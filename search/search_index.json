{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#news","title":"\ud83d\udd25 News","text":"<ul> <li>[August 3 2023] <code>ranx</code> <code>0.3.16</code> is out! This release adds support for importing Qrels and Runs from <code>parquet</code> files, exporting them as <code>pandas.DataFrame</code> and save them as <code>parquet</code> files. Any dependence on <code>trec_eval</code> have been removed to make <code>ranx</code> truly MIT-compliant.</li> </ul>"},{"location":"#introduction","title":"\u26a1\ufe0f Introduction","text":"<p>ranx ([ra\u014bks]) is a library of fast ranking evaluation metrics implemented in Python, leveraging Numba for high-speed vector operations and automatic parallelization. It offers a user-friendly interface to evaluate and compare Information Retrieval and Recommender Systems. ranx allows you to perform statistical tests and export LaTeX tables for your scientific publications. Moreover, ranx provides several fusion algorithms and normalization strategies, and an automatic fusion optimization functionality. ranx also have a companion repository of pre-computed runs to facilitated model comparisons called ranxhub. On ranxhub, you can download and share pre-computed runs for Information Retrieval datasets, such as MSMARCO Passage Ranking. ranx was featured in ECIR 2022, CIKM 2022, and SIGIR 2023. </p> <p>If you use ranx to evaluate results or conducting experiments involving fusion for your scientific publication, please consider citing it: evaluation bibtex, fusion bibtex, ranxhub bibtex.</p> <p>NB: <code>ranx</code> is not suited for evaluating classifiers. Please, refer to the FAQ for further details.</p> <p>For a quick overview, follow the Usage section.</p> <p>For a in-depth overview, follow the Examples section.</p>"},{"location":"#features","title":"\u2728 Features","text":""},{"location":"#metrics","title":"Metrics","text":"<ul> <li>Hits</li> <li>Hit Rate</li> <li>Precision</li> <li>Recall</li> <li>F1</li> <li>r-Precision</li> <li>Bpref</li> <li>Rank-biased Precision (RBP)</li> <li>Mean Reciprocal Rank (MRR)</li> <li>Mean Average Precision (MAP)</li> <li>Discounted Cumulative Gain (DCG)</li> <li>Normalized Discounted Cumulative Gain (NDCG)</li> </ul> <p>The metrics have been tested against TREC Eval for correctness.</p>"},{"location":"#statistical-tests","title":"Statistical Tests","text":"<ul> <li>Paired Student's t-Test (default)</li> <li>Fisher's Randomization Test</li> <li>Tukey's HSD Test</li> </ul> <p>Please, refer to Smucker et al., Carterette, and Fuhr for additional information on statistical tests for Information Retrieval.</p>"},{"location":"#off-the-shelf-qrels","title":"Off-the-shelf Qrels","text":"<p>You can load qrels from ir-datasets as simply as: <pre><code>qrels = Qrels.from_ir_datasets(\"msmarco-document/dev\")\n</code></pre> A full list of the available qrels is provided here.</p>"},{"location":"#off-the-shelf-runs","title":"Off-the-shelf Runs","text":"<p>You can load runs from ranxhub as simply as: <pre><code>run = Run.from_ranxhub(\"run-id\")\n</code></pre> A full list of the available runs is provided here.</p>"},{"location":"#fusion-algorithms","title":"Fusion Algorithms","text":"Name Name Name Name Name CombMIN CombMNZ RRF MAPFuse BordaFuse CombMED CombGMNZ RBC PosFuse Weighted BordaFuse CombANZ ISR WMNZ ProbFuse Condorcet CombMAX Log_ISR Mixed SegFuse Weighted Condorcet CombSUM LogN_ISR BayesFuse SlideFuse Weighted Sum <p>Please, refer to the documentation for further details.</p>"},{"location":"#normalization-strategies","title":"Normalization Strategies","text":"<ul> <li>Min-Max Norm </li> <li>Max Norm </li> <li>Sum Norm </li> <li>ZMUV Norm </li> <li>Rank Norm </li> <li>Borda Norm</li> </ul> <p>Please, refer to the documentation for further details.</p>"},{"location":"#requirements","title":"\ud83d\udd0c Requirements","text":"<p><pre><code>python&gt;=3.8\n</code></pre> As of <code>v.0.3.5</code>, ranx requires <code>python&gt;=3.8</code>.</p>"},{"location":"#installation","title":"\ud83d\udcbe Installation","text":"<pre><code>pip install ranx\n</code></pre>"},{"location":"#usage","title":"\ud83d\udca1 Usage","text":""},{"location":"#create-qrels-and-run","title":"Create Qrels and Run","text":"<pre><code>from ranx import Qrels, Run\n\nqrels_dict = { \"q_1\": { \"d_12\": 5, \"d_25\": 3 },\n               \"q_2\": { \"d_11\": 6, \"d_22\": 1 } }\n\nrun_dict = { \"q_1\": { \"d_12\": 0.9, \"d_23\": 0.8, \"d_25\": 0.7,\n                      \"d_36\": 0.6, \"d_32\": 0.5, \"d_35\": 0.4  },\n             \"q_2\": { \"d_12\": 0.9, \"d_11\": 0.8, \"d_25\": 0.7,\n                      \"d_36\": 0.6, \"d_22\": 0.5, \"d_35\": 0.4  } }\n\nqrels = Qrels(qrels_dict)\nrun = Run(run_dict)\n</code></pre>"},{"location":"#evaluate","title":"Evaluate","text":"<pre><code>from ranx import evaluate\n\n# Compute score for a single metric\nevaluate(qrels, run, \"ndcg@5\")\n&gt;&gt;&gt; 0.7861\n\n# Compute scores for multiple metrics at once\nevaluate(qrels, run, [\"map@5\", \"mrr\"])\n&gt;&gt;&gt; {\"map@5\": 0.6416, \"mrr\": 0.75}\n</code></pre>"},{"location":"#compare","title":"Compare","text":"<p><pre><code>from ranx import compare\n\n# Compare different runs and perform Two-sided Paired Student's t-Test\nreport = compare(\n    qrels=qrels,\n    runs=[run_1, run_2, run_3, run_4, run_5],\n    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n    max_p=0.01  # P-value threshold\n)\n</code></pre> Output: <pre><code>print(report)\n</code></pre> <pre><code>#    Model    MAP@100    MRR@100    NDCG@10\n---  -------  --------   --------   ---------\na    model_1  0.320\u1d47     0.320\u1d47     0.368\u1d47\u1d9c\nb    model_2  0.233      0.234      0.239\nc    model_3  0.308\u1d47     0.309\u1d47     0.330\u1d47\nd    model_4  0.366\u1d43\u1d47\u1d9c   0.367\u1d43\u1d47\u1d9c   0.408\u1d43\u1d47\u1d9c\ne    model_5  0.405\u1d43\u1d47\u1d9c\u1d48  0.406\u1d43\u1d47\u1d9c\u1d48  0.451\u1d43\u1d47\u1d9c\u1d48\n</code></pre></p>"},{"location":"#fusion","title":"Fusion","text":"<pre><code>from ranx import fuse, optimize_fusion\n\nbest_params = optimize_fusion(\n    qrels=train_qrels,\n    runs=[train_run_1, train_run_2, train_run_3],\n    norm=\"min-max\",     # The norm. to apply before fusion\n    method=\"wsum\",      # The fusion algorithm to use (Weighted Sum)\n    metric=\"ndcg@100\",  # The metric to maximize\n)\n\ncombined_test_run = fuse(\n    runs=[test_run_1, test_run_2, test_run_3],  \n    norm=\"min-max\",       \n    method=\"wsum\",        \n    params=best_params,\n)\n</code></pre>"},{"location":"#examples","title":"\ud83d\udcd6 Examples","text":"Name Link Overview Qrels and Run Evaluation Comparison and Report Fusion Plot Share your runs with ranxhub"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Browse the documentation for more details and examples.</p>"},{"location":"#citation","title":"\ud83c\udf93 Citation","text":"<p>If you use ranx to evaluate results for your scientific publication, please consider citing our ECIR 2022 paper:</p> BibTeX <pre><code>@inproceedings{ranx,\n  author       = {Elias Bassani},\n  title        = {ranx: {A} Blazing-Fast Python Library for Ranking Evaluation and Comparison},\n  booktitle    = {{ECIR} {(2)}},\n  series       = {Lecture Notes in Computer Science},\n  volume       = {13186},\n  pages        = {259--264},\n  publisher    = {Springer},\n  year         = {2022},\n  doi          = {10.1007/978-3-030-99739-7\\_30}\n}\n</code></pre> <p>If you use the fusion functionalities provided by ranx for conducting the experiments of your scientific publication, please consider citing our CIKM 2022 paper:</p> BibTeX <pre><code>@inproceedings{ranx.fuse,\n  author    = {Elias Bassani and\n              Luca Romelli},\n  title     = {ranx.fuse: {A} Python Library for Metasearch},\n  booktitle = {{CIKM}},\n  pages     = {4808--4812},\n  publisher = {{ACM}},\n  year      = {2022},\n  doi       = {10.1145/3511808.3557207}\n}\n</code></pre> <p>If you use pre-computed runs from [ranxhub]((https://amenra.github.io/ranxhub) to make comparison for your scientific publication, please consider citing our SIGIR 2023 paper:</p> BibTeX <pre><code>@inproceedings{ranxhub,\n  author       = {Elias Bassani},\n  title        = {ranxhub: An Online Repository for Information Retrieval Runs},\n  booktitle    = {{SIGIR}},\n  pages        = {3210--3214},\n  publisher    = {{ACM}},\n  year         = {2023},\n  doi          = {10.1145/3539618.3591823}\n}\n</code></pre>"},{"location":"#feature-requests","title":"\ud83c\udf81 Feature Requests","text":"<p>Would you like to see other features implemented? Please, open a feature request.</p>"},{"location":"#want-to-contribute","title":"\ud83e\udd18 Want to contribute?","text":"<p>Would you like to contribute? Please, drop me an e-mail.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>ranx is an open-sourced software licensed under the MIT license.</p>"},{"location":"compare/","title":"Compare","text":""},{"location":"compare/#ranx.compare.Qrels","title":"<code>Qrels</code>","text":"<p>             Bases: <code>object</code></p> <p><code>Qrels</code>, or query relevance judgments, stores the ground truth for conducting evaluations.</p> <p>The preferred way for creating a <code>Qrels</code> instance is converting Python dictionary as follows:</p> <pre><code>qrels_dict = {\n    \"q_1\": {\n        \"d_1\": 1,\n        \"d_2\": 2,\n    },\n    \"q_2\": {\n        \"d_3\": 2,\n        \"d_2\": 1,\n        \"d_5\": 3,\n    },\n}\n\nqrels = Qrels(qrels_dict, name=\"MSMARCO\")\n\nqrels = Qrels()  # Creates an empty Qrels with no name\n</code></pre> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>class Qrels(object):\n    \"\"\"`Qrels`, or _query relevance judgments_, stores the ground truth for conducting evaluations.\n\n    The preferred way for creating a `Qrels` instance is converting Python dictionary as follows:\n\n    ```python\n    qrels_dict = {\n        \"q_1\": {\n            \"d_1\": 1,\n            \"d_2\": 2,\n        },\n        \"q_2\": {\n            \"d_3\": 2,\n            \"d_2\": 1,\n            \"d_5\": 3,\n        },\n    }\n\n    qrels = Qrels(qrels_dict, name=\"MSMARCO\")\n\n    qrels = Qrels()  # Creates an empty Qrels with no name\n    ```\n    \"\"\"\n\n    def __init__(self, qrels: Dict[str, Dict[str, int]] = None, name: str = None):\n        if qrels is None:\n            self.qrels = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.DictType(types.unicode_type, types.int64),\n            )\n            self.sorted = False\n        else:\n            # Query IDs\n            q_ids = list(qrels.keys())\n            q_ids = TypedList(q_ids)\n\n            # Doc IDs\n            doc_ids = [list(doc.keys()) for doc in qrels.values()]\n            max_len = max(len(y) for x in doc_ids for y in x)\n            dtype = f\"&lt;U{max_len}\"\n            doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n            # Scores\n            scores = [list(doc.values()) for doc in qrels.values()]\n            scores = TypedList([np.array(x, dtype=int) for x in scores])\n\n            self.qrels = create_and_sort(q_ids, doc_ids, scores)\n            self.sorted = True\n\n        self.name = name\n\n    def keys(self):\n        \"\"\"Returns query ids. Used internally.\"\"\"\n        return self.qrels.keys()\n\n    def add_score(self, q_id: str, doc_id: str, score: int):\n        \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n        Args:\n            q_id (str): Query ID\n            doc_id (str): Document ID\n            score (int): Relevance score judgment\n        \"\"\"\n        if self.qrels.get(q_id) is None:\n            self.qrels[q_id] = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.int64,\n            )\n        self.qrels[q_id][doc_id] = int(score)\n        self.sorted = False\n\n    def add(self, q_id: str, doc_ids: List[str], scores: List[int]):\n        \"\"\"Add a query and its relevant documents with the associated relevance score judgment.\n\n        Args:\n            q_id (str): Query ID\n            doc_ids (List[str]): List of Document IDs\n            scores (List[int]): List of relevance score judgments\n        \"\"\"\n        self.add_multi([q_id], [doc_ids], [scores])\n\n    def add_multi(\n        self,\n        q_ids: List[str],\n        doc_ids: List[List[str]],\n        scores: List[List[int]],\n    ):\n        \"\"\"Add multiple queries at once.\n\n        Args:\n            q_ids (List[str]): List of Query IDs\n            doc_ids (List[List[str]]): List of list of Document IDs\n            scores (List[List[int]]): List of list of relevance score judgments\n        \"\"\"\n        q_ids = TypedList(q_ids)\n        doc_ids = TypedList([TypedList(x) for x in doc_ids])\n        scores = TypedList([TypedList(map(int, x)) for x in scores])\n\n        self.qrels = add_and_sort(self.qrels, q_ids, doc_ids, scores)\n        self.sorted = True\n\n    def set_relevance_level(self, rel_lvl: int = 1):\n        \"\"\"Sets relevance level.\"\"\"\n        self.qrels = _set_relevance_level(self.qrels, rel_lvl)\n\n    def get_query_ids(self):\n        \"\"\"Returns query ids.\"\"\"\n        return list(self.qrels.keys())\n\n    def get_doc_ids_and_scores(self):\n        \"\"\"Returns doc ids and relevance judgments.\"\"\"\n        return list(self.qrels.values())\n\n    # Sort in place\n    def sort(self):\n        \"\"\"Sort. Used internally.\"\"\"\n        self.qrels = sort_dict_by_key(self.qrels)\n        self.qrels = sort_dict_of_dict_by_value(self.qrels)\n        self.sorted = True\n\n    def to_typed_list(self):\n        \"\"\"Convert Qrels to Numba Typed List. Used internally.\"\"\"\n        if not self.sorted:\n            self.sort()\n        return to_typed_list(self.qrels)\n\n    def to_dict(self) -&gt; Dict[str, Dict[str, int]]:\n        \"\"\"Convert Qrels to Python dictionary.\n\n        Returns:\n            Dict[str, Dict[str, int]]: Qrels as Python dictionary\n        \"\"\"\n        d = defaultdict(dict)\n        for q_id in self.keys():\n            d[q_id] = dict(self[q_id])\n        return d\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert Qrels to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n        Returns:\n            pandas.DataFrame: Qrels as Pandas DataFrame.\n        \"\"\"\n        data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n        for q_id in self.qrels:\n            for doc_id in self.qrels[q_id]:\n                data[\"q_id\"].append(q_id)\n                data[\"doc_id\"].append(doc_id)\n                data[\"score\"].append(self.qrels[q_id][doc_id])\n\n        return pd.DataFrame.from_dict(data)\n\n    def save(self, path: str = \"qrels.json\", kind: str = None) -&gt; None:\n        \"\"\"Write `qrels` to `path` as JSON file, TREC qrels format, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str, optional): Saving path. Defaults to \"qrels.json\".\n            kind (str, optional): Kind of file to save, must be either \"json\" or \"trec\". If None, it will be automatically inferred from the filename extension.\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Save Qrels -----------------------------------------------------------\n        if kind == \"json\":\n            with open(path, \"wb\") as f:\n                f.write(orjson.dumps(self.to_dict(), option=orjson.OPT_INDENT_2))\n        elif kind == \"parquet\":\n            self.to_dataframe().to_parquet(path, index=False)\n        else:\n            with open(path, \"w\") as f:\n                for i, q_id in enumerate(self.qrels.keys()):\n                    for j, doc_id in enumerate(self.qrels[q_id].keys()):\n                        score = self.qrels[q_id][doc_id]\n                        f.write(f\"{q_id} 0 {doc_id} {score}\")\n\n                        if (\n                            i != len(self.qrels.keys()) - 1\n                            or j != len(self.qrels[q_id].keys()) - 1\n                        ):\n                            f.write(\"\\n\")\n\n    @staticmethod\n    def from_dict(d: Dict[str, Dict[str, int]]):\n        \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels.\n\n        Args:\n            d (Dict[str, Dict[str, int]]): Qrels as Python dictionary\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        # Query IDs\n        q_ids = list(d.keys())\n        q_ids = TypedList(q_ids)\n\n        # Doc IDs\n        doc_ids = [list(doc.keys()) for doc in d.values()]\n        max_len = max(len(y) for x in doc_ids for y in x)\n        dtype = f\"&lt;U{max_len}\"\n        doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n        # Scores\n        scores = [list(doc.values()) for doc in d.values()]\n        scores = TypedList([np.array(x, dtype=int) for x in scores])\n\n        qrels = Qrels()\n        qrels.qrels = create_and_sort(q_ids, doc_ids, scores)\n        qrels.sorted = True\n\n        return qrels\n\n    @staticmethod\n    def from_file(path: str, kind: str = None):\n        \"\"\"Parse a qrels file into ranx.Qrels. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str): File path.\n            kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Load Qrels -----------------------------------------------------------\n        if kind == \"json\":\n            qrels = orjson.loads(open(path, \"rb\").read())\n        else:\n            qrels = defaultdict(dict)\n            with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n                for line in f:\n                    q_id, _, doc_id, rel = line.split()\n                    qrels[q_id][doc_id] = int(rel)\n\n        return Qrels.from_dict(qrels)\n\n    @staticmethod\n    def from_df(\n        df: pd.DataFrame,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n    ):\n        \"\"\"Convert a Pandas DataFrame to ranx.Qrels.\n\n        Args:\n            df (pandas.DataFrame): Qrels as Pandas DataFrame.\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        assert (\n            df[q_id_col].dtype == \"O\"\n        ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n        assert (\n            df[doc_id_col].dtype == \"O\"\n        ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n        assert (\n            df[score_col].dtype == np.int64\n        ), \"DataFrame scores column dtype must be `int`\"\n\n        qrels_dict = (\n            df.groupby(q_id_col)[[doc_id_col, score_col]]\n            .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n            .to_dict()\n        )\n\n        return Qrels.from_dict(qrels_dict)\n\n    @staticmethod\n    def from_parquet(\n        path: str,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n        pd_kwargs: Dict[str, Any] = None,\n    ):\n        \"\"\"Convert a Parquet file to ranx.Qrels.\n\n        Args:\n            path (str): File path.\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n            pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n        return Qrels.from_df(\n            df=pd.read_parquet(path, *pd_kwargs),\n            q_id_col=q_id_col,\n            doc_id_col=doc_id_col,\n            score_col=score_col,\n        )\n\n    @staticmethod\n    def from_ir_datasets(dataset_id: str):\n        \"\"\"Convert `ir-datasets` qrels into ranx.Qrels. It automatically downloads data if missing.\n        Args:\n            dataset_id (str): ID of the detaset in `ir-datasets`. `ir-datasets` catalog is available here: https://ir-datasets.com/index.html.\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        qrels = Qrels.from_dict(ir_datasets.load(dataset_id).qrels_dict())\n        qrels.name = dataset_id\n        return qrels\n\n    @property\n    def size(self):\n        return len(self.qrels)\n\n    def __getitem__(self, q_id):\n        return dict(self.qrels[q_id])\n\n    def __len__(self) -&gt; int:\n        return len(self.qrels)\n\n    def __repr__(self):\n        return self.qrels.__repr__()\n\n    def __str__(self):\n        return self.qrels.__str__()\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.add","title":"<code>add(q_id, doc_ids, scores)</code>","text":"<p>Add a query and its relevant documents with the associated relevance score judgment.</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_ids</code> <code>List[str]</code> <p>List of Document IDs</p> required <code>scores</code> <code>List[int]</code> <p>List of relevance score judgments</p> required Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def add(self, q_id: str, doc_ids: List[str], scores: List[int]):\n    \"\"\"Add a query and its relevant documents with the associated relevance score judgment.\n\n    Args:\n        q_id (str): Query ID\n        doc_ids (List[str]): List of Document IDs\n        scores (List[int]): List of relevance score judgments\n    \"\"\"\n    self.add_multi([q_id], [doc_ids], [scores])\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.add_multi","title":"<code>add_multi(q_ids, doc_ids, scores)</code>","text":"<p>Add multiple queries at once.</p> <p>Parameters:</p> Name Type Description Default <code>q_ids</code> <code>List[str]</code> <p>List of Query IDs</p> required <code>doc_ids</code> <code>List[List[str]]</code> <p>List of list of Document IDs</p> required <code>scores</code> <code>List[List[int]]</code> <p>List of list of relevance score judgments</p> required Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def add_multi(\n    self,\n    q_ids: List[str],\n    doc_ids: List[List[str]],\n    scores: List[List[int]],\n):\n    \"\"\"Add multiple queries at once.\n\n    Args:\n        q_ids (List[str]): List of Query IDs\n        doc_ids (List[List[str]]): List of list of Document IDs\n        scores (List[List[int]]): List of list of relevance score judgments\n    \"\"\"\n    q_ids = TypedList(q_ids)\n    doc_ids = TypedList([TypedList(x) for x in doc_ids])\n    scores = TypedList([TypedList(map(int, x)) for x in scores])\n\n    self.qrels = add_and_sort(self.qrels, q_ids, doc_ids, scores)\n    self.sorted = True\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.add_score","title":"<code>add_score(q_id, doc_id, score)</code>","text":"<p>Add a (doc_id, score) pair to a query (or, change its value if it already exists).</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_id</code> <code>str</code> <p>Document ID</p> required <code>score</code> <code>int</code> <p>Relevance score judgment</p> required Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def add_score(self, q_id: str, doc_id: str, score: int):\n    \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n    Args:\n        q_id (str): Query ID\n        doc_id (str): Document ID\n        score (int): Relevance score judgment\n    \"\"\"\n    if self.qrels.get(q_id) is None:\n        self.qrels[q_id] = TypedDict.empty(\n            key_type=types.unicode_type,\n            value_type=types.int64,\n        )\n    self.qrels[q_id][doc_id] = int(score)\n    self.sorted = False\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.from_df","title":"<code>from_df(df, q_id_col='q_id', doc_id_col='doc_id', score_col='score')</code>  <code>staticmethod</code>","text":"<p>Convert a Pandas DataFrame to ranx.Qrels.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Qrels as Pandas DataFrame.</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance score judgments column. Defaults to \"score\".</p> <code>'score'</code> <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_df(\n    df: pd.DataFrame,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n):\n    \"\"\"Convert a Pandas DataFrame to ranx.Qrels.\n\n    Args:\n        df (pandas.DataFrame): Qrels as Pandas DataFrame.\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    assert (\n        df[q_id_col].dtype == \"O\"\n    ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n    assert (\n        df[doc_id_col].dtype == \"O\"\n    ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n    assert (\n        df[score_col].dtype == np.int64\n    ), \"DataFrame scores column dtype must be `int`\"\n\n    qrels_dict = (\n        df.groupby(q_id_col)[[doc_id_col, score_col]]\n        .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n        .to_dict()\n    )\n\n    return Qrels.from_dict(qrels_dict)\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.from_dict","title":"<code>from_dict(d)</code>  <code>staticmethod</code>","text":"<p>Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Dict[str, int]]</code> <p>Qrels as Python dictionary</p> required <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_dict(d: Dict[str, Dict[str, int]]):\n    \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels.\n\n    Args:\n        d (Dict[str, Dict[str, int]]): Qrels as Python dictionary\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    # Query IDs\n    q_ids = list(d.keys())\n    q_ids = TypedList(q_ids)\n\n    # Doc IDs\n    doc_ids = [list(doc.keys()) for doc in d.values()]\n    max_len = max(len(y) for x in doc_ids for y in x)\n    dtype = f\"&lt;U{max_len}\"\n    doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n    # Scores\n    scores = [list(doc.values()) for doc in d.values()]\n    scores = TypedList([np.array(x, dtype=int) for x in scores])\n\n    qrels = Qrels()\n    qrels.qrels = create_and_sort(q_ids, doc_ids, scores)\n    qrels.sorted = True\n\n    return qrels\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.from_file","title":"<code>from_file(path, kind=None)</code>  <code>staticmethod</code>","text":"<p>Parse a qrels file into ranx.Qrels. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>kind</code> <code>str</code> <p>Kind of file to load, must be either \"json\" or \"trec\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_file(path: str, kind: str = None):\n    \"\"\"Parse a qrels file into ranx.Qrels. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str): File path.\n        kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Load Qrels -----------------------------------------------------------\n    if kind == \"json\":\n        qrels = orjson.loads(open(path, \"rb\").read())\n    else:\n        qrels = defaultdict(dict)\n        with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n            for line in f:\n                q_id, _, doc_id, rel = line.split()\n                qrels[q_id][doc_id] = int(rel)\n\n    return Qrels.from_dict(qrels)\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.from_ir_datasets","title":"<code>from_ir_datasets(dataset_id)</code>  <code>staticmethod</code>","text":"<p>Convert <code>ir-datasets</code> qrels into ranx.Qrels. It automatically downloads data if missing. Args:     dataset_id (str): ID of the detaset in <code>ir-datasets</code>. <code>ir-datasets</code> catalog is available here: https://ir-datasets.com/index.html. Returns:     Qrels: ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_ir_datasets(dataset_id: str):\n    \"\"\"Convert `ir-datasets` qrels into ranx.Qrels. It automatically downloads data if missing.\n    Args:\n        dataset_id (str): ID of the detaset in `ir-datasets`. `ir-datasets` catalog is available here: https://ir-datasets.com/index.html.\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    qrels = Qrels.from_dict(ir_datasets.load(dataset_id).qrels_dict())\n    qrels.name = dataset_id\n    return qrels\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.from_parquet","title":"<code>from_parquet(path, q_id_col='q_id', doc_id_col='doc_id', score_col='score', pd_kwargs=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Parquet file to ranx.Qrels.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance score judgments column. Defaults to \"score\".</p> <code>'score'</code> <code>pd_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments to pass to <code>pandas.read_parquet</code> (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_parquet(\n    path: str,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n    pd_kwargs: Dict[str, Any] = None,\n):\n    \"\"\"Convert a Parquet file to ranx.Qrels.\n\n    Args:\n        path (str): File path.\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n        pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n    return Qrels.from_df(\n        df=pd.read_parquet(path, *pd_kwargs),\n        q_id_col=q_id_col,\n        doc_id_col=doc_id_col,\n        score_col=score_col,\n    )\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.get_doc_ids_and_scores","title":"<code>get_doc_ids_and_scores()</code>","text":"<p>Returns doc ids and relevance judgments.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def get_doc_ids_and_scores(self):\n    \"\"\"Returns doc ids and relevance judgments.\"\"\"\n    return list(self.qrels.values())\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.get_query_ids","title":"<code>get_query_ids()</code>","text":"<p>Returns query ids.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def get_query_ids(self):\n    \"\"\"Returns query ids.\"\"\"\n    return list(self.qrels.keys())\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.keys","title":"<code>keys()</code>","text":"<p>Returns query ids. Used internally.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def keys(self):\n    \"\"\"Returns query ids. Used internally.\"\"\"\n    return self.qrels.keys()\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.save","title":"<code>save(path='qrels.json', kind=None)</code>","text":"<p>Write <code>qrels</code> to <code>path</code> as JSON file, TREC qrels format, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Saving path. Defaults to \"qrels.json\".</p> <code>'qrels.json'</code> <code>kind</code> <code>str</code> <p>Kind of file to save, must be either \"json\" or \"trec\". If None, it will be automatically inferred from the filename extension.</p> <code>None</code> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def save(self, path: str = \"qrels.json\", kind: str = None) -&gt; None:\n    \"\"\"Write `qrels` to `path` as JSON file, TREC qrels format, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str, optional): Saving path. Defaults to \"qrels.json\".\n        kind (str, optional): Kind of file to save, must be either \"json\" or \"trec\". If None, it will be automatically inferred from the filename extension.\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Save Qrels -----------------------------------------------------------\n    if kind == \"json\":\n        with open(path, \"wb\") as f:\n            f.write(orjson.dumps(self.to_dict(), option=orjson.OPT_INDENT_2))\n    elif kind == \"parquet\":\n        self.to_dataframe().to_parquet(path, index=False)\n    else:\n        with open(path, \"w\") as f:\n            for i, q_id in enumerate(self.qrels.keys()):\n                for j, doc_id in enumerate(self.qrels[q_id].keys()):\n                    score = self.qrels[q_id][doc_id]\n                    f.write(f\"{q_id} 0 {doc_id} {score}\")\n\n                    if (\n                        i != len(self.qrels.keys()) - 1\n                        or j != len(self.qrels[q_id].keys()) - 1\n                    ):\n                        f.write(\"\\n\")\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.set_relevance_level","title":"<code>set_relevance_level(rel_lvl=1)</code>","text":"<p>Sets relevance level.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def set_relevance_level(self, rel_lvl: int = 1):\n    \"\"\"Sets relevance level.\"\"\"\n    self.qrels = _set_relevance_level(self.qrels, rel_lvl)\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.sort","title":"<code>sort()</code>","text":"<p>Sort. Used internally.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def sort(self):\n    \"\"\"Sort. Used internally.\"\"\"\n    self.qrels = sort_dict_by_key(self.qrels)\n    self.qrels = sort_dict_of_dict_by_value(self.qrels)\n    self.sorted = True\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert Qrels to Pandas DataFrame with the following columns: <code>q_id</code>, <code>doc_id</code>, and <code>score</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: Qrels as Pandas DataFrame.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert Qrels to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n    Returns:\n        pandas.DataFrame: Qrels as Pandas DataFrame.\n    \"\"\"\n    data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n    for q_id in self.qrels:\n        for doc_id in self.qrels[q_id]:\n            data[\"q_id\"].append(q_id)\n            data[\"doc_id\"].append(doc_id)\n            data[\"score\"].append(self.qrels[q_id][doc_id])\n\n    return pd.DataFrame.from_dict(data)\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert Qrels to Python dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, int]]</code> <p>Dict[str, Dict[str, int]]: Qrels as Python dictionary</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Dict[str, int]]:\n    \"\"\"Convert Qrels to Python dictionary.\n\n    Returns:\n        Dict[str, Dict[str, int]]: Qrels as Python dictionary\n    \"\"\"\n    d = defaultdict(dict)\n    for q_id in self.keys():\n        d[q_id] = dict(self[q_id])\n    return d\n</code></pre>"},{"location":"compare/#ranx.compare.Qrels.to_typed_list","title":"<code>to_typed_list()</code>","text":"<p>Convert Qrels to Numba Typed List. Used internally.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def to_typed_list(self):\n    \"\"\"Convert Qrels to Numba Typed List. Used internally.\"\"\"\n    if not self.sorted:\n        self.sort()\n    return to_typed_list(self.qrels)\n</code></pre>"},{"location":"compare/#ranx.compare.Report","title":"<code>Report</code>","text":"<p>             Bases: <code>object</code></p> <p>A <code>Report</code> instance is automatically generated as the results of a comparison. A <code>Report</code> provide a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications.</p> <p><pre><code># Compare different runs and perform statistical tests\nreport = compare(\n    qrels=qrels,\n    runs=[run_1, run_2, run_3, run_4, run_5],\n    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n    max_p=0.01  # P-value threshold\n)\n\nprint(report)\n</code></pre> Output: <pre><code>#    Model    MAP@100     MRR@100     NDCG@10\n---  -------  ----------  ----------  ----------\na    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\nb    model_2  0.2332      0.2339      0.239\nc    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\nd    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\ne    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n</code></pre> <pre><code>print(report.to_latex())  # To get the LaTeX code\n</code></pre></p> Source code in <code>ranx/data_structures/report.py</code> <pre><code>class Report(object):\n    \"\"\"A `Report` instance is automatically generated as the results of a comparison.\n    A `Report` provide a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications.\n\n    ```python\n    # Compare different runs and perform statistical tests\n    report = compare(\n        qrels=qrels,\n        runs=[run_1, run_2, run_3, run_4, run_5],\n        metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n        max_p=0.01  # P-value threshold\n    )\n\n    print(report)\n    ```\n    Output:\n    ```\n    #    Model    MAP@100     MRR@100     NDCG@10\n    ---  -------  ----------  ----------  ----------\n    a    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\n    b    model_2  0.2332      0.2339      0.239\n    c    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\n    d    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\n    e    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n    ```\n    ```python\n    print(report.to_latex())  # To get the LaTeX code\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_names: List[str],\n        results: Dict,\n        comparisons: FrozensetDict,\n        metrics: List[str],\n        max_p: float,\n        win_tie_loss: Dict[Tuple[str], Dict[str, Dict[str, int]]],\n        rounding_digits: int = 3,\n        show_percentages: bool = False,\n        stat_test: str = \"student\",\n    ):\n        self.model_names = model_names\n        self.results = results\n        self.comparisons = comparisons\n        self.metrics = metrics\n        self.max_p = max_p\n        self.win_tie_loss = win_tie_loss\n        self.rounding_digits = rounding_digits\n        self.show_percentages = show_percentages\n        self.stat_test = stat_test\n\n    def format_score(self, score):\n        if self.show_percentages:\n            new_score = round(score * 100, max(0, self.rounding_digits - 2))\n            return \"%.{n}f\".format(n=self.rounding_digits - 2) % new_score\n        new_score = round(score, self.rounding_digits)\n        return \"%.{n}f\".format(n=self.rounding_digits) % new_score\n\n    def get_superscript_for_table(self, model, metric):\n        superscript = [\n            super_chars[j]\n            for j, _model in enumerate(self.model_names)\n            if model != _model\n            and self.comparisons[model, _model][metric][\"significant\"]\n            and (self.results[model][metric] &gt; self.results[_model][metric])\n        ]\n        return (\"\").join(superscript)\n\n    def get_metric_label(self, m):\n        if \"-l\" in m:\n            m, rel_lvl = m.split(\"-l\")\n            if \"@\" in m:\n                m_split = m.split(\"@\")\n                label = metric_labels[m_split[0]]\n                cutoff = m_split[1]\n                return f\"{label}@{cutoff}-l{rel_lvl}\"\n            return f\"{metric_labels[m]}-l{rel_lvl}\"\n\n        else:\n            if \"@\" in m:\n                m_split = m.split(\"@\")\n                label = metric_labels[m_split[0]]\n                cutoff = m_split[1]\n                return f\"{label}@{cutoff}\"\n            return f\"{metric_labels[m]}\"\n\n    def get_stat_test_label(self, stat_test: str):\n        return stat_test_labels[stat_test]\n\n    def to_table(self):\n        tabular_data = []\n\n        for i, (run, v) in enumerate(self.results.items()):\n            data = [chars[i], run]\n\n            for metric, score in v.items():\n                formatted_score = self.format_score(score)\n                superscript = self.get_superscript_for_table(run, metric)\n                data.append(f\"{formatted_score}{superscript}\")\n\n            tabular_data.append(data)\n\n        headers = [\"#\", \"Model\"]\n\n        for x in self.metrics:\n            label = self.get_metric_label(x)\n            headers.append(label)\n\n        return tabulate(tabular_data=tabular_data, headers=headers)\n\n    def get_superscript_for_latex(self, model, metric):\n        superscript = [\n            chars[j]\n            for j, _model in enumerate(self.model_names)\n            if (\n                model != _model\n                and self.comparisons[model, _model][metric][\"significant\"]\n                and self.results[model][metric] &gt; self.results[_model][metric]\n            )\n        ]\n        return (\"\").join(superscript)\n\n    def get_phantoms_for_latex(self, model, metric):\n        phantoms = [\n            chars[j]\n            for j, _model in enumerate(self.model_names)\n            if (\n                model != _model\n                and (\n                    not self.comparisons[model, _model][metric][\"significant\"]\n                    or not self.results[model][metric] &gt; self.results[_model][metric]\n                )\n            )\n        ]\n\n        if len(phantoms) &gt; 0:\n            return (\"\").join(phantoms)\n\n        return \"\"\n\n    def to_latex(self) -&gt; str:\n        \"\"\"Returns Report as LaTeX table.\n\n        Returns:\n            str: LaTeX table\n        \"\"\"\n        best_scores = {}\n\n        for m in self.metrics:\n            best_model = None\n            best_score = 0.0\n            for model in self.model_names:\n                if best_score &lt; round(self.results[model][m], self.rounding_digits):\n                    best_score = round(self.results[model][m], self.rounding_digits)\n                    best_model = model\n            best_scores[m] = best_model\n\n        preamble = \"========================\\n% Add in preamble\\n\\\\usepackage{graphicx}\\n\\\\usepackage{booktabs}\\n========================\\n\\n\"\n\n        table_prefix = (\n            \"% To change the table size, act on the resizebox argument `0.8`.\\n\"\n            + \"\"\"\\\\begin{table*}[ht]\\n\\centering\\n\\caption{\\nOverall effectiveness of the models.\\nThe best results are highlighted in boldface.\\nSuperscripts denote significant differences in \"\"\"\n            + self.get_stat_test_label(self.stat_test)\n            + \"\"\" with $p \\le \"\"\"\n            + str(self.max_p)\n            + \"$.\\n}\\n\\\\resizebox{0.8\\\\textwidth}{!}{\"\n            + \"\\n\\\\begin{tabular}{c|l\"\n            + \"|c\" * len(self.metrics)\n            + \"}\"\n            + \"\\n\\\\toprule\"\n            + \"\\n\\\\textbf{\\#}\"\n            + \"\\n&amp; \\\\textbf{Model}\"\n            + \"\".join(\n                [f\"\\n&amp; \\\\textbf{{{self.get_metric_label(m)}}}\" for m in self.metrics]\n            )\n            + \" \\\\\\\\ \\n\\midrule\"\n        )\n\n        table_content = []\n\n        for i, model in enumerate(self.model_names):\n            table_raw = f\"{chars[i]} &amp;\\n\" + f\"{model} &amp;\\n\"\n            scores = []\n\n            for m in self.metrics:\n                score = self.format_score(self.results[model][m])\n                score = (\n                    f\"\\\\textbf{{{score}}}\" if best_scores[m] == model else f\"{score}\"\n                )\n                superscript = self.get_superscript_for_latex(model, m)\n                phantoms = self.get_phantoms_for_latex(model, m)\n                scores.append(\n                    f\"{score}$^{{{superscript}}}$\\\\hphantom{{$^{{{phantoms}}}$}} &amp;\"\n                )\n\n            scores[-1] = scores[-1][:-1]  # Remove `&amp;` at the end\n\n            table_raw += \"\\n\".join(scores) + \"\\\\\\\\\"\n            table_content.append(table_raw)\n\n        table_content = (\n            \"\\n\".join(table_content).replace(\"_\", \"\\\\_\").replace(\"$^{}$\", \"\")\n        )\n\n        table_suffix = (\n            \"\\\\bottomrule\\n\\end{tabular}\\n}\\n\\label{tab:results}\\n\\end{table*}\"\n        )\n\n        return (\n            preamble + \"\\n\" + table_prefix + \"\\n\" + table_content + \"\\n\" + table_suffix\n        )\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Returns the Report data as a Python dictionary.\n\n        ```python\n        {\n            \"stat_test\": \"fisher\"\n            # metrics and model_names allows to read the report without\n            # inspecting the json to discover the used metrics and\n            # the compared models\n            \"metrics\": [\"metric_1\", \"metric_2\", ...],\n            \"model_names\": [\"model_1\", \"model_2\", ...],\n            #\n            \"model_1\": {\n                \"scores\": {\n                    \"metric_1\": ...,\n                    \"metric_2\": ...,\n                    ...\n                },\n                \"comparisons\": {\n                    \"model_2\": {\n                        \"metric_1\": ...,  # p-value\n                        \"metric_2\": ...,  # p-value\n                        ...\n                    },\n                    ...\n                },\n                \"win_tie_loss\": {\n                    \"model_2\": {\n                        \"W\": ...,\n                        \"T\": ...,\n                        \"L\": ...,\n                    },\n                    ...\n                },\n            },\n            ...\n        }\n        ```\n\n        Returns:\n            Dict: Report data as a Python dictionary\n        \"\"\"\n\n        d = {\n            \"stat_test\": self.stat_test,\n            \"metrics\": self.metrics,\n            \"model_names\": self.model_names,\n        }\n\n        for m1 in self.model_names:\n            d[m1] = {}\n            d[m1][\"scores\"] = self.results[m1]\n            d[m1][\"comparisons\"] = {}\n            d[m1][\"win_tie_loss\"] = {}\n\n            for m2 in self.model_names:\n                if m1 != m2:\n                    d[m1][\"comparisons\"][m2] = {}\n                    d[m1][\"win_tie_loss\"][m2] = {}\n\n                    for metric in self.metrics:\n                        d[m1][\"comparisons\"][m2][metric] = self.comparisons[{m1, m2}][\n                            metric\n                        ][\"p_value\"]\n                        d[m1][\"win_tie_loss\"][m2][metric] = self.win_tie_loss[(m1, m2)][\n                            metric\n                        ]\n\n        return d\n\n    def save(self, path: str):\n        \"\"\"Save the Report data as JSON file.\n        See [**Report.to_dict**][ranx.report.to_dict] for more details.\n\n        Args:\n            path (str): Saving path\n        \"\"\"\n        with open(path, \"w\") as f:\n            f.write(json.dumps(self.to_dict(), indent=4))\n\n    def print_results(self):\n        \"\"\"Print report data.\"\"\"\n        print(json.dumps(self.results, indent=4))\n\n    def __repr__(self):\n        return self.to_table()\n\n    def __str__(self):\n        return self.to_table()\n</code></pre>"},{"location":"compare/#ranx.compare.Report.print_results","title":"<code>print_results()</code>","text":"<p>Print report data.</p> Source code in <code>ranx/data_structures/report.py</code> <pre><code>def print_results(self):\n    \"\"\"Print report data.\"\"\"\n    print(json.dumps(self.results, indent=4))\n</code></pre>"},{"location":"compare/#ranx.compare.Report.save","title":"<code>save(path)</code>","text":"<p>Save the Report data as JSON file. See Report.to_dict for more details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Saving path</p> required Source code in <code>ranx/data_structures/report.py</code> <pre><code>def save(self, path: str):\n    \"\"\"Save the Report data as JSON file.\n    See [**Report.to_dict**][ranx.report.to_dict] for more details.\n\n    Args:\n        path (str): Saving path\n    \"\"\"\n    with open(path, \"w\") as f:\n        f.write(json.dumps(self.to_dict(), indent=4))\n</code></pre>"},{"location":"compare/#ranx.compare.Report.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns the Report data as a Python dictionary.</p> <pre><code>{\n    \"stat_test\": \"fisher\"\n    # metrics and model_names allows to read the report without\n    # inspecting the json to discover the used metrics and\n    # the compared models\n    \"metrics\": [\"metric_1\", \"metric_2\", ...],\n    \"model_names\": [\"model_1\", \"model_2\", ...],\n    #\n    \"model_1\": {\n        \"scores\": {\n            \"metric_1\": ...,\n            \"metric_2\": ...,\n            ...\n        },\n        \"comparisons\": {\n            \"model_2\": {\n                \"metric_1\": ...,  # p-value\n                \"metric_2\": ...,  # p-value\n                ...\n            },\n            ...\n        },\n        \"win_tie_loss\": {\n            \"model_2\": {\n                \"W\": ...,\n                \"T\": ...,\n                \"L\": ...,\n            },\n            ...\n        },\n    },\n    ...\n}\n</code></pre> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Report data as a Python dictionary</p> Source code in <code>ranx/data_structures/report.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Returns the Report data as a Python dictionary.\n\n    ```python\n    {\n        \"stat_test\": \"fisher\"\n        # metrics and model_names allows to read the report without\n        # inspecting the json to discover the used metrics and\n        # the compared models\n        \"metrics\": [\"metric_1\", \"metric_2\", ...],\n        \"model_names\": [\"model_1\", \"model_2\", ...],\n        #\n        \"model_1\": {\n            \"scores\": {\n                \"metric_1\": ...,\n                \"metric_2\": ...,\n                ...\n            },\n            \"comparisons\": {\n                \"model_2\": {\n                    \"metric_1\": ...,  # p-value\n                    \"metric_2\": ...,  # p-value\n                    ...\n                },\n                ...\n            },\n            \"win_tie_loss\": {\n                \"model_2\": {\n                    \"W\": ...,\n                    \"T\": ...,\n                    \"L\": ...,\n                },\n                ...\n            },\n        },\n        ...\n    }\n    ```\n\n    Returns:\n        Dict: Report data as a Python dictionary\n    \"\"\"\n\n    d = {\n        \"stat_test\": self.stat_test,\n        \"metrics\": self.metrics,\n        \"model_names\": self.model_names,\n    }\n\n    for m1 in self.model_names:\n        d[m1] = {}\n        d[m1][\"scores\"] = self.results[m1]\n        d[m1][\"comparisons\"] = {}\n        d[m1][\"win_tie_loss\"] = {}\n\n        for m2 in self.model_names:\n            if m1 != m2:\n                d[m1][\"comparisons\"][m2] = {}\n                d[m1][\"win_tie_loss\"][m2] = {}\n\n                for metric in self.metrics:\n                    d[m1][\"comparisons\"][m2][metric] = self.comparisons[{m1, m2}][\n                        metric\n                    ][\"p_value\"]\n                    d[m1][\"win_tie_loss\"][m2][metric] = self.win_tie_loss[(m1, m2)][\n                        metric\n                    ]\n\n    return d\n</code></pre>"},{"location":"compare/#ranx.compare.Report.to_latex","title":"<code>to_latex()</code>","text":"<p>Returns Report as LaTeX table.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>LaTeX table</p> Source code in <code>ranx/data_structures/report.py</code> <pre><code>def to_latex(self) -&gt; str:\n    \"\"\"Returns Report as LaTeX table.\n\n    Returns:\n        str: LaTeX table\n    \"\"\"\n    best_scores = {}\n\n    for m in self.metrics:\n        best_model = None\n        best_score = 0.0\n        for model in self.model_names:\n            if best_score &lt; round(self.results[model][m], self.rounding_digits):\n                best_score = round(self.results[model][m], self.rounding_digits)\n                best_model = model\n        best_scores[m] = best_model\n\n    preamble = \"========================\\n% Add in preamble\\n\\\\usepackage{graphicx}\\n\\\\usepackage{booktabs}\\n========================\\n\\n\"\n\n    table_prefix = (\n        \"% To change the table size, act on the resizebox argument `0.8`.\\n\"\n        + \"\"\"\\\\begin{table*}[ht]\\n\\centering\\n\\caption{\\nOverall effectiveness of the models.\\nThe best results are highlighted in boldface.\\nSuperscripts denote significant differences in \"\"\"\n        + self.get_stat_test_label(self.stat_test)\n        + \"\"\" with $p \\le \"\"\"\n        + str(self.max_p)\n        + \"$.\\n}\\n\\\\resizebox{0.8\\\\textwidth}{!}{\"\n        + \"\\n\\\\begin{tabular}{c|l\"\n        + \"|c\" * len(self.metrics)\n        + \"}\"\n        + \"\\n\\\\toprule\"\n        + \"\\n\\\\textbf{\\#}\"\n        + \"\\n&amp; \\\\textbf{Model}\"\n        + \"\".join(\n            [f\"\\n&amp; \\\\textbf{{{self.get_metric_label(m)}}}\" for m in self.metrics]\n        )\n        + \" \\\\\\\\ \\n\\midrule\"\n    )\n\n    table_content = []\n\n    for i, model in enumerate(self.model_names):\n        table_raw = f\"{chars[i]} &amp;\\n\" + f\"{model} &amp;\\n\"\n        scores = []\n\n        for m in self.metrics:\n            score = self.format_score(self.results[model][m])\n            score = (\n                f\"\\\\textbf{{{score}}}\" if best_scores[m] == model else f\"{score}\"\n            )\n            superscript = self.get_superscript_for_latex(model, m)\n            phantoms = self.get_phantoms_for_latex(model, m)\n            scores.append(\n                f\"{score}$^{{{superscript}}}$\\\\hphantom{{$^{{{phantoms}}}$}} &amp;\"\n            )\n\n        scores[-1] = scores[-1][:-1]  # Remove `&amp;` at the end\n\n        table_raw += \"\\n\".join(scores) + \"\\\\\\\\\"\n        table_content.append(table_raw)\n\n    table_content = (\n        \"\\n\".join(table_content).replace(\"_\", \"\\\\_\").replace(\"$^{}$\", \"\")\n    )\n\n    table_suffix = (\n        \"\\\\bottomrule\\n\\end{tabular}\\n}\\n\\label{tab:results}\\n\\end{table*}\"\n    )\n\n    return (\n        preamble + \"\\n\" + table_prefix + \"\\n\" + table_content + \"\\n\" + table_suffix\n    )\n</code></pre>"},{"location":"compare/#ranx.compare.Run","title":"<code>Run</code>","text":"<p>             Bases: <code>object</code></p> <p><code>Run</code> stores the relevance scores estimated by the model under evaluation.&lt;\br&gt; The preferred way for creating a <code>Run</code> instance is converting a Python dictionary as follows:</p> <pre><code>run_dict = {\n    \"q_1\": {\n        \"d_1\": 1.5,\n        \"d_2\": 2.6,\n    },\n    \"q_2\": {\n        \"d_3\": 2.8,\n        \"d_2\": 1.2,\n        \"d_5\": 3.1,\n    },\n}\n\nrun = Run(run_dict, name=\"bm25\")\n\nrun = Run()  # Creates an empty Run with no name\n</code></pre> Source code in <code>ranx/data_structures/run.py</code> <pre><code>class Run(object):\n    \"\"\"`Run` stores the relevance scores estimated by the model under evaluation.&lt;\\br&gt;\n    The preferred way for creating a `Run` instance is converting a Python dictionary as follows:\n\n    ```python\n    run_dict = {\n        \"q_1\": {\n            \"d_1\": 1.5,\n            \"d_2\": 2.6,\n        },\n        \"q_2\": {\n            \"d_3\": 2.8,\n            \"d_2\": 1.2,\n            \"d_5\": 3.1,\n        },\n    }\n\n    run = Run(run_dict, name=\"bm25\")\n\n    run = Run()  # Creates an empty Run with no name\n    ```\n    \"\"\"\n\n    def __init__(self, run: Dict[str, Dict[str, float]] = None, name: str = None):\n        if run is None:\n            self.run = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.DictType(types.unicode_type, types.float64),\n            )\n            self.sorted = False\n        else:\n            # Query IDs\n            q_ids = list(run.keys())\n            q_ids = TypedList(q_ids)\n\n            # Doc IDs\n            doc_ids = [list(doc.keys()) for doc in run.values()]\n            max_len = max(len(y) for x in doc_ids for y in x)\n            dtype = f\"&lt;U{max_len}\"\n            doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n            # Scores\n            scores = [list(doc.values()) for doc in run.values()]\n            scores = TypedList([np.array(x, dtype=float) for x in scores])\n            self.run = create_and_sort(q_ids, doc_ids, scores)\n            self.sorted = True\n\n        self.name = name\n        self.metadata = {}\n        self.scores = defaultdict(dict)\n        self.mean_scores = {}\n        self.std_scores = {}\n\n    def keys(self):\n        \"\"\"Returns query ids. Used internally.\"\"\"\n        return self.run.keys()\n\n    def add_score(self, q_id: str, doc_id: str, score: int):\n        \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n        Args:\n            q_id (str): Query ID\n            doc_id (str): Document ID\n            score (int): Relevance score\n        \"\"\"\n        if self.run.get(q_id) is None:\n            self.run[q_id] = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.float64,\n            )\n        self.run[q_id][doc_id] = float(score)\n        self.sorted = False\n\n    def add(self, q_id: str, doc_ids: List[str], scores: List[float]):\n        \"\"\"Add a query and its relevant documents with the associated relevance score.\n\n        Args:\n            q_id (str): Query ID\n            doc_ids (List[str]): List of Document IDs\n            scores (List[int]): List of relevance scores\n        \"\"\"\n        self.add_multi([q_id], [doc_ids], [scores])\n\n    def add_multi(\n        self,\n        q_ids: List[str],\n        doc_ids: List[List[str]],\n        scores: List[List[float]],\n    ):\n        \"\"\"Add multiple queries at once.\n\n        Args:\n            q_ids (List[str]): List of Query IDs\n            doc_ids (List[List[str]]): List of list of Document IDs\n            scores (List[List[int]]): List of list of relevance scores\n        \"\"\"\n        q_ids = TypedList(q_ids)\n        doc_ids = TypedList([TypedList(x) for x in doc_ids])\n        scores = TypedList([TypedList(map(float, x)) for x in scores])\n\n        self.run = add_and_sort(self.run, q_ids, doc_ids, scores)\n        self.sorted = True\n\n    def get_query_ids(self):\n        \"\"\"Returns query ids.\"\"\"\n        return list(self.run.keys())\n\n    def get_doc_ids_and_scores(self):\n        \"\"\"Returns doc ids and relevance scores.\"\"\"\n        return list(self.run.values())\n\n    # Sort in place\n    def sort(self):\n        \"\"\"Sort. Used internally.\"\"\"\n        self.run = sort_dict_by_key(self.run)\n        self.run = sort_dict_of_dict_by_value(self.run)\n        self.sorted = True\n\n    def make_comparable(self, qrels: Qrels):\n        \"\"\"Adds empty results for queries missing from the run and removes those not appearing in qrels.\"\"\"\n        # Adds empty results for missing queries\n        for q_id in qrels.qrels:\n            if q_id not in self.run:\n                self.run[q_id] = create_empty_results_dict()\n\n        # Remove results for additional queries\n        for q_id in self.run:\n            if q_id not in qrels.qrels:\n                del self.run[q_id]\n\n        self.sort()\n\n        return self\n\n    def to_typed_list(self):\n        \"\"\"Convert Run to Numba Typed List. Used internally.\"\"\"\n        if not self.sorted:\n            self.sort()\n        return to_typed_list(self.run)\n\n    def to_dict(self):\n        \"\"\"Convert Run to Python dictionary.\n\n        Returns:\n            Dict[str, Dict[str, int]]: Run as Python dictionary\n        \"\"\"\n        d = defaultdict(dict)\n        for q_id in self.keys():\n            d[q_id] = dict(self[q_id])\n        return d\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert Run to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n        Returns:\n            pandas.DataFrame: Run as Pandas DataFrame.\n        \"\"\"\n        data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n        for q_id in self.run:\n            for doc_id in self.run[q_id]:\n                data[\"q_id\"].append(q_id)\n                data[\"doc_id\"].append(doc_id)\n                data[\"score\"].append(self.run[q_id][doc_id])\n\n        return pd.DataFrame.from_dict(data)\n\n    def save(self, path: str = \"run.json\", kind: str = None):\n        \"\"\"Write `run` to `path` as JSON file, TREC run, LZ4 file, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", and \".lz4\" -&gt; \"lz4\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str, optional): Saving path. Defaults to \"run.json\".\n            kind (str, optional): Kind of file to save, must be either \"json\", \"trec\", or \"ranxhub\". If None, it will be automatically inferred from the filename extension.\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Save Run -------------------------------------------------------------\n        if not self.sorted:\n            self.sort()\n\n        if kind == \"json\":\n            save_json(self.to_dict(), path)\n        elif kind == \"lz4\":\n            save_lz4(self.to_dict(), path)\n        elif kind == \"parquet\":\n            self.to_dataframe().to_parquet(path, index=False)\n        else:\n            with open(path, \"w\") as f:\n                for i, q_id in enumerate(self.run.keys()):\n                    for rank, doc_id in enumerate(self.run[q_id].keys()):\n                        score = self.run[q_id][doc_id]\n                        f.write(f\"{q_id} Q0 {doc_id} {rank+1} {score} {self.name}\")\n\n                        if (\n                            i != len(self.run.keys()) - 1\n                            or rank != len(self.run[q_id].keys()) - 1\n                        ):\n                            f.write(\"\\n\")\n\n    @staticmethod\n    def from_dict(d: Dict[str, Dict[str, float]], name: str = None):\n        \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run.\n\n        Args:\n            d (Dict[str, Dict[str, int]]): Run as Python dictionary\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n\n        # Query IDs\n        q_ids = list(d.keys())\n        q_ids = TypedList(q_ids)\n\n        # Doc IDs\n        doc_ids = [list(doc.keys()) for doc in d.values()]\n        max_len = max(len(y) for x in doc_ids for y in x)\n        dtype = f\"&lt;U{max_len}\"\n        doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n        # Scores\n        scores = [list(doc.values()) for doc in d.values()]\n        scores = TypedList([np.array(x, dtype=float) for x in scores])\n\n        run = Run()\n        run.run = create_and_sort(q_ids, doc_ids, scores)\n        run.sorted = True\n        run.name = name\n\n        return run\n\n    @staticmethod\n    def from_file(path: str, kind: str = None, name: str = None):\n        \"\"\"Parse a run file into ranx.Run. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\", \".lz4\" -&gt; \"lz4\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str): File path.\n            kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Load Run -------------------------------------------------------------\n        if kind == \"json\":\n            run = load_json(path)\n        elif kind == \"lz4\":\n            run = load_lz4(path)\n        else:\n            run = defaultdict(dict)\n            with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n                for line in f:\n                    q_id, _, doc_id, _, rel, run_name = line.split()\n                    run[q_id][doc_id] = float(rel)\n                    if name is None:\n                        name = run_name\n\n        run = Run.from_dict(run, name)\n\n        return run\n\n    @staticmethod\n    def from_df(\n        df: pd.DataFrame,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n        name: str = None,\n    ):\n        \"\"\"Convert a Pandas DataFrame to ranx.Run.\n\n        Args:\n            df (pd.DataFrame): Run as Pandas DataFrame\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance scores column. Defaults to \"score\".\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        assert (\n            df[q_id_col].dtype == \"O\"\n        ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n        assert (\n            df[doc_id_col].dtype == \"O\"\n        ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n        assert (\n            df[score_col].dtype == np.float64\n        ), \"DataFrame scores column dtype must be `float`\"\n\n        run_py = (\n            df.groupby(q_id_col)[[doc_id_col, score_col]]\n            .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n            .to_dict()\n        )\n\n        return Run.from_dict(run_py, name)\n\n    @staticmethod\n    def from_parquet(\n        path: str,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n        pd_kwargs: Dict[str, Any] = None,\n        name: str = None,\n    ):\n        \"\"\"Convert a Parquet file to ranx.Run.\n\n        Args:\n            path (str): File path.\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance scores column. Defaults to \"score\".\n            pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n        return Run.from_df(\n            df=pd.read_parquet(path, *pd_kwargs),\n            q_id_col=q_id_col,\n            doc_id_col=doc_id_col,\n            score_col=score_col,\n            name=name,\n        )\n\n    @staticmethod\n    def from_ranxhub(id: str):\n        \"\"\"Download and load a ranx.Run from ranxhub.\n\n        Args:\n            path (str): Run ID.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        content = download(id)\n\n        run = Run.from_dict(content[\"run\"])\n        run.name = content[\"metadata\"][\"run\"][\"name\"]\n        run.metadata = content[\"metadata\"]\n\n        return run\n\n    @property\n    def size(self):\n        return len(self.run)\n\n    def __getitem__(self, q_id):\n        return dict(self.run[q_id])\n\n    def __len__(self) -&gt; int:\n        return len(self.run)\n\n    def __repr__(self):\n        return self.run.__repr__()\n\n    def __str__(self):\n        return self.run.__str__()\n</code></pre>"},{"location":"compare/#ranx.compare.Run.add","title":"<code>add(q_id, doc_ids, scores)</code>","text":"<p>Add a query and its relevant documents with the associated relevance score.</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_ids</code> <code>List[str]</code> <p>List of Document IDs</p> required <code>scores</code> <code>List[int]</code> <p>List of relevance scores</p> required Source code in <code>ranx/data_structures/run.py</code> <pre><code>def add(self, q_id: str, doc_ids: List[str], scores: List[float]):\n    \"\"\"Add a query and its relevant documents with the associated relevance score.\n\n    Args:\n        q_id (str): Query ID\n        doc_ids (List[str]): List of Document IDs\n        scores (List[int]): List of relevance scores\n    \"\"\"\n    self.add_multi([q_id], [doc_ids], [scores])\n</code></pre>"},{"location":"compare/#ranx.compare.Run.add_multi","title":"<code>add_multi(q_ids, doc_ids, scores)</code>","text":"<p>Add multiple queries at once.</p> <p>Parameters:</p> Name Type Description Default <code>q_ids</code> <code>List[str]</code> <p>List of Query IDs</p> required <code>doc_ids</code> <code>List[List[str]]</code> <p>List of list of Document IDs</p> required <code>scores</code> <code>List[List[int]]</code> <p>List of list of relevance scores</p> required Source code in <code>ranx/data_structures/run.py</code> <pre><code>def add_multi(\n    self,\n    q_ids: List[str],\n    doc_ids: List[List[str]],\n    scores: List[List[float]],\n):\n    \"\"\"Add multiple queries at once.\n\n    Args:\n        q_ids (List[str]): List of Query IDs\n        doc_ids (List[List[str]]): List of list of Document IDs\n        scores (List[List[int]]): List of list of relevance scores\n    \"\"\"\n    q_ids = TypedList(q_ids)\n    doc_ids = TypedList([TypedList(x) for x in doc_ids])\n    scores = TypedList([TypedList(map(float, x)) for x in scores])\n\n    self.run = add_and_sort(self.run, q_ids, doc_ids, scores)\n    self.sorted = True\n</code></pre>"},{"location":"compare/#ranx.compare.Run.add_score","title":"<code>add_score(q_id, doc_id, score)</code>","text":"<p>Add a (doc_id, score) pair to a query (or, change its value if it already exists).</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_id</code> <code>str</code> <p>Document ID</p> required <code>score</code> <code>int</code> <p>Relevance score</p> required Source code in <code>ranx/data_structures/run.py</code> <pre><code>def add_score(self, q_id: str, doc_id: str, score: int):\n    \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n    Args:\n        q_id (str): Query ID\n        doc_id (str): Document ID\n        score (int): Relevance score\n    \"\"\"\n    if self.run.get(q_id) is None:\n        self.run[q_id] = TypedDict.empty(\n            key_type=types.unicode_type,\n            value_type=types.float64,\n        )\n    self.run[q_id][doc_id] = float(score)\n    self.sorted = False\n</code></pre>"},{"location":"compare/#ranx.compare.Run.from_df","title":"<code>from_df(df, q_id_col='q_id', doc_id_col='doc_id', score_col='score', name=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Pandas DataFrame to ranx.Run.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Run as Pandas DataFrame</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance scores column. Defaults to \"score\".</p> <code>'score'</code> <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_df(\n    df: pd.DataFrame,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n    name: str = None,\n):\n    \"\"\"Convert a Pandas DataFrame to ranx.Run.\n\n    Args:\n        df (pd.DataFrame): Run as Pandas DataFrame\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance scores column. Defaults to \"score\".\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    assert (\n        df[q_id_col].dtype == \"O\"\n    ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n    assert (\n        df[doc_id_col].dtype == \"O\"\n    ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n    assert (\n        df[score_col].dtype == np.float64\n    ), \"DataFrame scores column dtype must be `float`\"\n\n    run_py = (\n        df.groupby(q_id_col)[[doc_id_col, score_col]]\n        .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n        .to_dict()\n    )\n\n    return Run.from_dict(run_py, name)\n</code></pre>"},{"location":"compare/#ranx.compare.Run.from_dict","title":"<code>from_dict(d, name=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Dict[str, int]]</code> <p>Run as Python dictionary</p> required <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_dict(d: Dict[str, Dict[str, float]], name: str = None):\n    \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run.\n\n    Args:\n        d (Dict[str, Dict[str, int]]): Run as Python dictionary\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n\n    # Query IDs\n    q_ids = list(d.keys())\n    q_ids = TypedList(q_ids)\n\n    # Doc IDs\n    doc_ids = [list(doc.keys()) for doc in d.values()]\n    max_len = max(len(y) for x in doc_ids for y in x)\n    dtype = f\"&lt;U{max_len}\"\n    doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n    # Scores\n    scores = [list(doc.values()) for doc in d.values()]\n    scores = TypedList([np.array(x, dtype=float) for x in scores])\n\n    run = Run()\n    run.run = create_and_sort(q_ids, doc_ids, scores)\n    run.sorted = True\n    run.name = name\n\n    return run\n</code></pre>"},{"location":"compare/#ranx.compare.Run.from_file","title":"<code>from_file(path, kind=None, name=None)</code>  <code>staticmethod</code>","text":"<p>Parse a run file into ranx.Run. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\", \".lz4\" -&gt; \"lz4\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>kind</code> <code>str</code> <p>Kind of file to load, must be either \"json\" or \"trec\".</p> <code>None</code> <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_file(path: str, kind: str = None, name: str = None):\n    \"\"\"Parse a run file into ranx.Run. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\", \".lz4\" -&gt; \"lz4\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str): File path.\n        kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Load Run -------------------------------------------------------------\n    if kind == \"json\":\n        run = load_json(path)\n    elif kind == \"lz4\":\n        run = load_lz4(path)\n    else:\n        run = defaultdict(dict)\n        with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n            for line in f:\n                q_id, _, doc_id, _, rel, run_name = line.split()\n                run[q_id][doc_id] = float(rel)\n                if name is None:\n                    name = run_name\n\n    run = Run.from_dict(run, name)\n\n    return run\n</code></pre>"},{"location":"compare/#ranx.compare.Run.from_parquet","title":"<code>from_parquet(path, q_id_col='q_id', doc_id_col='doc_id', score_col='score', pd_kwargs=None, name=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Parquet file to ranx.Run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance scores column. Defaults to \"score\".</p> <code>'score'</code> <code>pd_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments to pass to <code>pandas.read_parquet</code> (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_parquet(\n    path: str,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n    pd_kwargs: Dict[str, Any] = None,\n    name: str = None,\n):\n    \"\"\"Convert a Parquet file to ranx.Run.\n\n    Args:\n        path (str): File path.\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance scores column. Defaults to \"score\".\n        pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n    return Run.from_df(\n        df=pd.read_parquet(path, *pd_kwargs),\n        q_id_col=q_id_col,\n        doc_id_col=doc_id_col,\n        score_col=score_col,\n        name=name,\n    )\n</code></pre>"},{"location":"compare/#ranx.compare.Run.from_ranxhub","title":"<code>from_ranxhub(id)</code>  <code>staticmethod</code>","text":"<p>Download and load a ranx.Run from ranxhub.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Run ID.</p> required <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_ranxhub(id: str):\n    \"\"\"Download and load a ranx.Run from ranxhub.\n\n    Args:\n        path (str): Run ID.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    content = download(id)\n\n    run = Run.from_dict(content[\"run\"])\n    run.name = content[\"metadata\"][\"run\"][\"name\"]\n    run.metadata = content[\"metadata\"]\n\n    return run\n</code></pre>"},{"location":"compare/#ranx.compare.Run.get_doc_ids_and_scores","title":"<code>get_doc_ids_and_scores()</code>","text":"<p>Returns doc ids and relevance scores.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def get_doc_ids_and_scores(self):\n    \"\"\"Returns doc ids and relevance scores.\"\"\"\n    return list(self.run.values())\n</code></pre>"},{"location":"compare/#ranx.compare.Run.get_query_ids","title":"<code>get_query_ids()</code>","text":"<p>Returns query ids.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def get_query_ids(self):\n    \"\"\"Returns query ids.\"\"\"\n    return list(self.run.keys())\n</code></pre>"},{"location":"compare/#ranx.compare.Run.keys","title":"<code>keys()</code>","text":"<p>Returns query ids. Used internally.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def keys(self):\n    \"\"\"Returns query ids. Used internally.\"\"\"\n    return self.run.keys()\n</code></pre>"},{"location":"compare/#ranx.compare.Run.make_comparable","title":"<code>make_comparable(qrels)</code>","text":"<p>Adds empty results for queries missing from the run and removes those not appearing in qrels.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def make_comparable(self, qrels: Qrels):\n    \"\"\"Adds empty results for queries missing from the run and removes those not appearing in qrels.\"\"\"\n    # Adds empty results for missing queries\n    for q_id in qrels.qrels:\n        if q_id not in self.run:\n            self.run[q_id] = create_empty_results_dict()\n\n    # Remove results for additional queries\n    for q_id in self.run:\n        if q_id not in qrels.qrels:\n            del self.run[q_id]\n\n    self.sort()\n\n    return self\n</code></pre>"},{"location":"compare/#ranx.compare.Run.save","title":"<code>save(path='run.json', kind=None)</code>","text":"<p>Write <code>run</code> to <code>path</code> as JSON file, TREC run, LZ4 file, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", and \".lz4\" -&gt; \"lz4\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Saving path. Defaults to \"run.json\".</p> <code>'run.json'</code> <code>kind</code> <code>str</code> <p>Kind of file to save, must be either \"json\", \"trec\", or \"ranxhub\". If None, it will be automatically inferred from the filename extension.</p> <code>None</code> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def save(self, path: str = \"run.json\", kind: str = None):\n    \"\"\"Write `run` to `path` as JSON file, TREC run, LZ4 file, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", and \".lz4\" -&gt; \"lz4\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str, optional): Saving path. Defaults to \"run.json\".\n        kind (str, optional): Kind of file to save, must be either \"json\", \"trec\", or \"ranxhub\". If None, it will be automatically inferred from the filename extension.\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Save Run -------------------------------------------------------------\n    if not self.sorted:\n        self.sort()\n\n    if kind == \"json\":\n        save_json(self.to_dict(), path)\n    elif kind == \"lz4\":\n        save_lz4(self.to_dict(), path)\n    elif kind == \"parquet\":\n        self.to_dataframe().to_parquet(path, index=False)\n    else:\n        with open(path, \"w\") as f:\n            for i, q_id in enumerate(self.run.keys()):\n                for rank, doc_id in enumerate(self.run[q_id].keys()):\n                    score = self.run[q_id][doc_id]\n                    f.write(f\"{q_id} Q0 {doc_id} {rank+1} {score} {self.name}\")\n\n                    if (\n                        i != len(self.run.keys()) - 1\n                        or rank != len(self.run[q_id].keys()) - 1\n                    ):\n                        f.write(\"\\n\")\n</code></pre>"},{"location":"compare/#ranx.compare.Run.sort","title":"<code>sort()</code>","text":"<p>Sort. Used internally.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def sort(self):\n    \"\"\"Sort. Used internally.\"\"\"\n    self.run = sort_dict_by_key(self.run)\n    self.run = sort_dict_of_dict_by_value(self.run)\n    self.sorted = True\n</code></pre>"},{"location":"compare/#ranx.compare.Run.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert Run to Pandas DataFrame with the following columns: <code>q_id</code>, <code>doc_id</code>, and <code>score</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: Run as Pandas DataFrame.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert Run to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n    Returns:\n        pandas.DataFrame: Run as Pandas DataFrame.\n    \"\"\"\n    data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n    for q_id in self.run:\n        for doc_id in self.run[q_id]:\n            data[\"q_id\"].append(q_id)\n            data[\"doc_id\"].append(doc_id)\n            data[\"score\"].append(self.run[q_id][doc_id])\n\n    return pd.DataFrame.from_dict(data)\n</code></pre>"},{"location":"compare/#ranx.compare.Run.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert Run to Python dictionary.</p> <p>Returns:</p> Type Description <p>Dict[str, Dict[str, int]]: Run as Python dictionary</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def to_dict(self):\n    \"\"\"Convert Run to Python dictionary.\n\n    Returns:\n        Dict[str, Dict[str, int]]: Run as Python dictionary\n    \"\"\"\n    d = defaultdict(dict)\n    for q_id in self.keys():\n        d[q_id] = dict(self[q_id])\n    return d\n</code></pre>"},{"location":"compare/#ranx.compare.Run.to_typed_list","title":"<code>to_typed_list()</code>","text":"<p>Convert Run to Numba Typed List. Used internally.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def to_typed_list(self):\n    \"\"\"Convert Run to Numba Typed List. Used internally.\"\"\"\n    if not self.sorted:\n        self.sort()\n    return to_typed_list(self.run)\n</code></pre>"},{"location":"compare/#ranx.compare.compare","title":"<code>compare(qrels, runs, metrics, stat_test='student', n_permutations=1000, max_p=0.01, random_seed=42, threads=0, rounding_digits=3, show_percentages=False, make_comparable=False)</code>","text":"<p>Evaluate multiple <code>runs</code> and compute statistical tests.</p> <p>Usage example: <pre><code>from ranx import compare\n\n# Compare different runs and perform statistical tests\nreport = compare(\n    qrels=qrels,\n    runs=[run_1, run_2, run_3, run_4, run_5],\n    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n    max_p=0.01  # P-value threshold\n)\n\nprint(report)\n</code></pre> Output: <pre><code>#    Model    MAP@100     MRR@100     NDCG@10\n---  -------  ----------  ----------  ----------\na    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\nb    model_2  0.2332      0.2339      0.239\nc    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\nd    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\ne    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>qrels</code> <code>Qrels</code> <p>Qrels.</p> required <code>runs</code> <code>List[Run]</code> <p>List of runs.</p> required <code>metrics</code> <code>Union[List[str], str]</code> <p>Metric or list of metrics.</p> required <code>n_permutations</code> <code>int</code> <p>Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000.</p> <code>1000</code> <code>max_p</code> <code>float</code> <p>Maximum p-value to consider an increment as statistically significant. Defaults to 0.01.</p> <code>0.01</code> <code>stat_test</code> <code>str</code> <p>Statistical test to perform. Use \"fisher\" for Fisher's Randomization Test, \"student\" for Two-sided Paired Student's t-Test, or \"Tukey\" for Tukey's HSD test. Defaults to \"student\".</p> <code>'student'</code> <code>random_seed</code> <code>int</code> <p>Random seed to use for generating the permutations. Defaults to 42.</p> <code>42</code> <code>threads</code> <code>int</code> <p>Number of threads to use, zero means all the available threads. Defaults to 0.</p> <code>0</code> <code>rounding_digits</code> <code>int</code> <p>Number of digits to round to and to show in the Report. Defaults to 3.</p> <code>3</code> <code>show_percentages</code> <code>bool</code> <p>Whether to show percentages instead of floats in the Report. Defaults to False.</p> <code>False</code> <code>make_comparable</code> <code>bool</code> <p>Adds empty results for queries missing from the runs and removes those not appearing in qrels. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Report</code> <code>Report</code> <p>See report.</p> Source code in <code>ranx/meta/compare.py</code> <pre><code>def compare(\n    qrels: Qrels,\n    runs: List[Run],\n    metrics: Union[List[str], str],\n    stat_test: str = \"student\",\n    n_permutations: int = 1000,\n    max_p: float = 0.01,\n    random_seed: int = 42,\n    threads: int = 0,\n    rounding_digits: int = 3,\n    show_percentages: bool = False,\n    make_comparable: bool = False,\n) -&gt; Report:\n    \"\"\"Evaluate multiple `runs` and compute statistical tests.\n\n    Usage example:\n    ```python\n    from ranx import compare\n\n    # Compare different runs and perform statistical tests\n    report = compare(\n        qrels=qrels,\n        runs=[run_1, run_2, run_3, run_4, run_5],\n        metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n        max_p=0.01  # P-value threshold\n    )\n\n    print(report)\n    ```\n    Output:\n    ```\n    #    Model    MAP@100     MRR@100     NDCG@10\n    ---  -------  ----------  ----------  ----------\n    a    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\n    b    model_2  0.2332      0.2339      0.239\n    c    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\n    d    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\n    e    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n    ```\n\n    Args:\n        qrels (Qrels): Qrels.\n        runs (List[Run]): List of runs.\n        metrics (Union[List[str], str]): Metric or list of metrics.\n        n_permutations (int, optional): Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000.\n        max_p (float, optional): Maximum p-value to consider an increment as statistically significant. Defaults to 0.01.\n        stat_test (str, optional): Statistical test to perform. Use \"fisher\" for _Fisher's Randomization Test_, \"student\" for _Two-sided Paired Student's t-Test_, or \"Tukey\" for _Tukey's HSD test_. Defaults to \"student\".\n        random_seed (int, optional): Random seed to use for generating the permutations. Defaults to 42.\n        threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.\n        rounding_digits (int, optional): Number of digits to round to and to show in the Report. Defaults to 3.\n        show_percentages (bool, optional): Whether to show percentages instead of floats in the Report. Defaults to False.\n        make_comparable (bool, optional): Adds empty results for queries missing from the runs and removes those not appearing in qrels. Defaults to False.\n\n    Returns:\n        Report: See report.\n    \"\"\"\n    metrics = format_metrics(metrics)\n    assert all(isinstance(m, str) for m in metrics), \"Metrics error\"\n\n    model_names = []\n    results = defaultdict(dict)\n\n    metric_scores = {}\n\n    # Compute scores for each run for each query -------------------------------\n    for i, run in enumerate(runs):\n        model_name = run.name if run.name is not None else f\"run_{i+1}\"\n        model_names.append(model_name)\n\n        metric_scores[model_name] = evaluate(\n            qrels=qrels,\n            run=run,\n            metrics=metrics,\n            return_mean=False,\n            threads=threads,\n            make_comparable=make_comparable,\n        )\n\n        if len(metrics) == 1:\n            metric_scores[model_name] = {metrics[0]: metric_scores[model_name]}\n\n        for m in metrics:\n            results[model_name][m] = float(np.mean(metric_scores[model_name][m]))\n\n    # Run statistical testing --------------------------------------------------\n    comparisons = compute_statistical_significance(\n        model_names=model_names,\n        metric_scores=metric_scores,\n        stat_test=stat_test,\n        n_permutations=n_permutations,\n        max_p=max_p,\n        random_seed=random_seed,\n    )\n\n    # Compute win / tie / lose -------------------------------------------------\n    win_tie_loss = defaultdict(dict)\n\n    for control in model_names:\n        for treatment in model_names:\n            if control != treatment:\n                for m in metrics:\n                    control_scores = metric_scores[control][m]\n                    treatment_scores = metric_scores[treatment][m]\n                    win_tie_loss[(control, treatment)][m] = {\n                        \"W\": int(sum(control_scores &gt; treatment_scores)),\n                        \"T\": int(sum(control_scores == treatment_scores)),\n                        \"L\": int(sum(control_scores &lt; treatment_scores)),\n                    }\n\n    return Report(\n        model_names=model_names,\n        results=dict(results),\n        comparisons=comparisons,\n        metrics=metrics,\n        max_p=max_p,\n        win_tie_loss=dict(win_tie_loss),\n        rounding_digits=rounding_digits,\n        show_percentages=show_percentages,\n        stat_test=stat_test,\n    )\n</code></pre>"},{"location":"compare/#ranx.compare.compute_statistical_significance","title":"<code>compute_statistical_significance(model_names, metric_scores, stat_test='fisher', n_permutations=1000, max_p=0.01, random_seed=42)</code>","text":"<p>Used internally.</p> Source code in <code>ranx/statistical_tests/__init__.py</code> <pre><code>def compute_statistical_significance(\n    model_names: List[str],\n    metric_scores: Dict[str, Dict[str, np.ndarray]],\n    stat_test: str = \"fisher\",\n    n_permutations: int = 1000,\n    max_p: float = 0.01,\n    random_seed: int = 42,\n):\n    \"\"\"Used internally.\"\"\"\n    comparisons = FrozensetDict()\n\n    if stat_test in {\"fisher\", \"student\"}:\n        for control in model_names:\n            control_metric_scores = metric_scores[control]\n            for treatment in model_names:\n                if control != treatment:\n                    treatment_metric_scores = metric_scores[treatment]\n\n                    # Compute statistical significance\n                    comparisons[\n                        frozenset([control, treatment])\n                    ] = _compute_statistical_significance(\n                        control_metric_scores,\n                        treatment_metric_scores,\n                        stat_test,\n                        n_permutations,\n                        max_p,\n                        random_seed,\n                    )\n\n        return comparisons\n    elif stat_test in {\"tukey\"}:\n        metrics = list(metric_scores[model_names[0]])\n\n        # Initialize comparisons\n        for i, control in enumerate(model_names):\n            for treatment in model_names[i:]:\n                comparisons[frozenset([control, treatment])] = {\n                    m: None for m in metrics\n                }\n\n        for m in metrics:\n            scores = [metric_scores[name][m] for name in model_names]\n            results = tukey_hsd_test(\n                model_names=model_names,\n                scores=scores,\n                max_p=max_p,\n            )\n\n            for res in results:\n                comparisons[res[\"control\"], res[\"treatment\"]][m] = {\n                    \"p_value\": res[\"p-value\"],\n                    \"significant\": res[\"significant\"],\n                }\n\n    else:\n        raise NotImplementedError(f\"Statistical test `{stat_test}` not supported.\")\n\n    return comparisons\n</code></pre>"},{"location":"compare/#ranx.compare.evaluate","title":"<code>evaluate(qrels, run, metrics, return_mean=True, return_std=False, threads=0, save_results_in_run=True, make_comparable=False)</code>","text":"<p>Compute the performance scores for the provided <code>qrels</code> and <code>run</code> for all the specified metrics.</p> <p>Usage examples:</p> <p>from ranx import evaluate</p>"},{"location":"compare/#ranx.compare.evaluate--compute-score-for-a-single-metric","title":"Compute score for a single metric","text":"<p>evaluate(qrels, run, \"ndcg@5\")</p> <p>0.7861</p>"},{"location":"compare/#ranx.compare.evaluate--compute-scores-for-multiple-metrics-at-once","title":"Compute scores for multiple metrics at once","text":"<p>evaluate(qrels, run, [\"map@5\", \"mrr\"])</p> <p>{\"map@5\": 0.6416, \"mrr\": 0.75}</p>"},{"location":"compare/#ranx.compare.evaluate--computed-metric-scores-are-saved-in-the-run-object","title":"Computed metric scores are saved in the Run object","text":"<p>run.mean_scores</p> <p>{\"ndcg@5\": 0.7861, \"map@5\": 0.6416, \"mrr\": 0.75}</p>"},{"location":"compare/#ranx.compare.evaluate--access-scores-for-each-query","title":"Access scores for each query","text":"<p>dict(run.scores)</p> <p>{ ...     \"ndcg@5\": {\"q_1\": 0.9430, \"q_2\": 0.6292}, ...     \"map@5\": {\"q_1\": 0.8333, \"q_2\": 0.4500}, ...     \"mrr\": {\"q_1\": 1.0000, \"q_2\": 0.5000}, ... } Args:     qrels (Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Qrels.     run (Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Run.     metrics (Union[List[str], str]): Metrics or list of metric to compute.     return_mean (bool, optional): Whether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True.     threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.     save_results_in_run (bool, optional): Save metric scores for each query in the input <code>run</code>. Defaults to True.     make_comparable (bool, optional): Adds empty results for queries missing from the run and removes those not appearing in qrels. Defaults to False.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, float], float]</code> <p>Union[Dict[str, float], float]: Results.</p> Source code in <code>ranx/meta/evaluate.py</code> <pre><code>def evaluate(\n    qrels: Union[\n        Qrels,\n        Dict[str, Dict[str, Number]],\n        nb.typed.typedlist.List,\n        np.ndarray,\n    ],\n    run: Union[\n        Run,\n        Dict[str, Dict[str, Number]],\n        nb.typed.typedlist.List,\n        np.ndarray,\n    ],\n    metrics: Union[List[str], str],\n    return_mean: bool = True,\n    return_std: bool = False,\n    threads: int = 0,\n    save_results_in_run: bool = True,\n    make_comparable: bool = False,\n) -&gt; Union[Dict[str, float], float]:\n    \"\"\"Compute the performance scores for the provided `qrels` and `run` for all the specified metrics.\n\n    Usage examples:\n\n    from ranx import evaluate\n\n    # Compute score for a single metric\n    evaluate(qrels, run, \"ndcg@5\")\n    &gt;&gt;&gt; 0.7861\n\n    # Compute scores for multiple metrics at once\n    evaluate(qrels, run, [\"map@5\", \"mrr\"])\n    &gt;&gt;&gt; {\"map@5\": 0.6416, \"mrr\": 0.75}\n\n    # Computed metric scores are saved in the Run object\n    run.mean_scores\n    &gt;&gt;&gt; {\"ndcg@5\": 0.7861, \"map@5\": 0.6416, \"mrr\": 0.75}\n\n    # Access scores for each query\n    dict(run.scores)\n    &gt;&gt;&gt; {\n    ...     \"ndcg@5\": {\"q_1\": 0.9430, \"q_2\": 0.6292},\n    ...     \"map@5\": {\"q_1\": 0.8333, \"q_2\": 0.4500},\n    ...     \"mrr\": {\"q_1\": 1.0000, \"q_2\": 0.5000},\n    ... }\n    Args:\n        qrels (Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Qrels.\n        run (Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Run.\n        metrics (Union[List[str], str]): Metrics or list of metric to compute.\n        return_mean (bool, optional): Whether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True.\n        threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.\n        save_results_in_run (bool, optional): Save metric scores for each query in the input `run`. Defaults to True.\n        make_comparable (bool, optional): Adds empty results for queries missing from the run and removes those not appearing in qrels. Defaults to False.\n\n    Returns:\n        Union[Dict[str, float], float]: Results.\n    \"\"\"\n\n    if len(qrels) &lt; 10:\n        set_num_threads(1)\n    elif threads != 0:\n        set_num_threads(threads)\n\n    if not return_mean:\n        return_std = False\n\n    if make_comparable and type(qrels) == Qrels and type(run) == Run:\n        run = run.make_comparable(qrels)\n\n    if type(qrels) in [Qrels, dict] and type(run) in [Run, dict]:\n        check_keys(qrels, run)\n\n    _qrels = convert_qrels(qrels)\n    _run = convert_run(run)\n    metrics = format_metrics(metrics)\n    assert all(isinstance(m, str) for m in metrics), \"Metrics error\"\n\n    # Compute metrics ----------------------------------------------------------\n    metric_scores_dict = {}\n    for metric in metrics:\n        m, k, rel_lvl = extract_metric_and_params(metric)\n        metric_scores_dict[metric] = metric_switch(m)(_qrels, _run, k, rel_lvl)\n\n    # Save results in Run ------------------------------------------------------\n    if type(run) == Run and save_results_in_run:\n        for m, scores in metric_scores_dict.items():\n            run.mean_scores[m] = np.mean(scores)\n            if return_std:\n                run.std_scores[m] = np.std(scores)\n            for i, q_id in enumerate(run.get_query_ids()):\n                run.scores[m][q_id] = scores[i]\n\n    # Prepare output -----------------------------------------------------------\n    if return_mean:\n        for m, scores in metric_scores_dict.items():\n            if return_std:\n                metric_scores_dict[m] = {\n                    \"mean\": np.mean(scores),\n                    \"std\": np.std(scores),\n                }\n\n            else:\n                metric_scores_dict[m] = np.mean(scores)\n\n    return metric_scores_dict[m] if len(metrics) == 1 else metric_scores_dict\n</code></pre>"},{"location":"evaluate/","title":"Evaluate","text":""},{"location":"evaluate/#ranx.evaluate.Qrels","title":"<code>Qrels</code>","text":"<p>             Bases: <code>object</code></p> <p><code>Qrels</code>, or query relevance judgments, stores the ground truth for conducting evaluations.</p> <p>The preferred way for creating a <code>Qrels</code> instance is converting Python dictionary as follows:</p> <pre><code>qrels_dict = {\n    \"q_1\": {\n        \"d_1\": 1,\n        \"d_2\": 2,\n    },\n    \"q_2\": {\n        \"d_3\": 2,\n        \"d_2\": 1,\n        \"d_5\": 3,\n    },\n}\n\nqrels = Qrels(qrels_dict, name=\"MSMARCO\")\n\nqrels = Qrels()  # Creates an empty Qrels with no name\n</code></pre> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>class Qrels(object):\n    \"\"\"`Qrels`, or _query relevance judgments_, stores the ground truth for conducting evaluations.\n\n    The preferred way for creating a `Qrels` instance is converting Python dictionary as follows:\n\n    ```python\n    qrels_dict = {\n        \"q_1\": {\n            \"d_1\": 1,\n            \"d_2\": 2,\n        },\n        \"q_2\": {\n            \"d_3\": 2,\n            \"d_2\": 1,\n            \"d_5\": 3,\n        },\n    }\n\n    qrels = Qrels(qrels_dict, name=\"MSMARCO\")\n\n    qrels = Qrels()  # Creates an empty Qrels with no name\n    ```\n    \"\"\"\n\n    def __init__(self, qrels: Dict[str, Dict[str, int]] = None, name: str = None):\n        if qrels is None:\n            self.qrels = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.DictType(types.unicode_type, types.int64),\n            )\n            self.sorted = False\n        else:\n            # Query IDs\n            q_ids = list(qrels.keys())\n            q_ids = TypedList(q_ids)\n\n            # Doc IDs\n            doc_ids = [list(doc.keys()) for doc in qrels.values()]\n            max_len = max(len(y) for x in doc_ids for y in x)\n            dtype = f\"&lt;U{max_len}\"\n            doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n            # Scores\n            scores = [list(doc.values()) for doc in qrels.values()]\n            scores = TypedList([np.array(x, dtype=int) for x in scores])\n\n            self.qrels = create_and_sort(q_ids, doc_ids, scores)\n            self.sorted = True\n\n        self.name = name\n\n    def keys(self):\n        \"\"\"Returns query ids. Used internally.\"\"\"\n        return self.qrels.keys()\n\n    def add_score(self, q_id: str, doc_id: str, score: int):\n        \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n        Args:\n            q_id (str): Query ID\n            doc_id (str): Document ID\n            score (int): Relevance score judgment\n        \"\"\"\n        if self.qrels.get(q_id) is None:\n            self.qrels[q_id] = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.int64,\n            )\n        self.qrels[q_id][doc_id] = int(score)\n        self.sorted = False\n\n    def add(self, q_id: str, doc_ids: List[str], scores: List[int]):\n        \"\"\"Add a query and its relevant documents with the associated relevance score judgment.\n\n        Args:\n            q_id (str): Query ID\n            doc_ids (List[str]): List of Document IDs\n            scores (List[int]): List of relevance score judgments\n        \"\"\"\n        self.add_multi([q_id], [doc_ids], [scores])\n\n    def add_multi(\n        self,\n        q_ids: List[str],\n        doc_ids: List[List[str]],\n        scores: List[List[int]],\n    ):\n        \"\"\"Add multiple queries at once.\n\n        Args:\n            q_ids (List[str]): List of Query IDs\n            doc_ids (List[List[str]]): List of list of Document IDs\n            scores (List[List[int]]): List of list of relevance score judgments\n        \"\"\"\n        q_ids = TypedList(q_ids)\n        doc_ids = TypedList([TypedList(x) for x in doc_ids])\n        scores = TypedList([TypedList(map(int, x)) for x in scores])\n\n        self.qrels = add_and_sort(self.qrels, q_ids, doc_ids, scores)\n        self.sorted = True\n\n    def set_relevance_level(self, rel_lvl: int = 1):\n        \"\"\"Sets relevance level.\"\"\"\n        self.qrels = _set_relevance_level(self.qrels, rel_lvl)\n\n    def get_query_ids(self):\n        \"\"\"Returns query ids.\"\"\"\n        return list(self.qrels.keys())\n\n    def get_doc_ids_and_scores(self):\n        \"\"\"Returns doc ids and relevance judgments.\"\"\"\n        return list(self.qrels.values())\n\n    # Sort in place\n    def sort(self):\n        \"\"\"Sort. Used internally.\"\"\"\n        self.qrels = sort_dict_by_key(self.qrels)\n        self.qrels = sort_dict_of_dict_by_value(self.qrels)\n        self.sorted = True\n\n    def to_typed_list(self):\n        \"\"\"Convert Qrels to Numba Typed List. Used internally.\"\"\"\n        if not self.sorted:\n            self.sort()\n        return to_typed_list(self.qrels)\n\n    def to_dict(self) -&gt; Dict[str, Dict[str, int]]:\n        \"\"\"Convert Qrels to Python dictionary.\n\n        Returns:\n            Dict[str, Dict[str, int]]: Qrels as Python dictionary\n        \"\"\"\n        d = defaultdict(dict)\n        for q_id in self.keys():\n            d[q_id] = dict(self[q_id])\n        return d\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert Qrels to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n        Returns:\n            pandas.DataFrame: Qrels as Pandas DataFrame.\n        \"\"\"\n        data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n        for q_id in self.qrels:\n            for doc_id in self.qrels[q_id]:\n                data[\"q_id\"].append(q_id)\n                data[\"doc_id\"].append(doc_id)\n                data[\"score\"].append(self.qrels[q_id][doc_id])\n\n        return pd.DataFrame.from_dict(data)\n\n    def save(self, path: str = \"qrels.json\", kind: str = None) -&gt; None:\n        \"\"\"Write `qrels` to `path` as JSON file, TREC qrels format, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str, optional): Saving path. Defaults to \"qrels.json\".\n            kind (str, optional): Kind of file to save, must be either \"json\" or \"trec\". If None, it will be automatically inferred from the filename extension.\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Save Qrels -----------------------------------------------------------\n        if kind == \"json\":\n            with open(path, \"wb\") as f:\n                f.write(orjson.dumps(self.to_dict(), option=orjson.OPT_INDENT_2))\n        elif kind == \"parquet\":\n            self.to_dataframe().to_parquet(path, index=False)\n        else:\n            with open(path, \"w\") as f:\n                for i, q_id in enumerate(self.qrels.keys()):\n                    for j, doc_id in enumerate(self.qrels[q_id].keys()):\n                        score = self.qrels[q_id][doc_id]\n                        f.write(f\"{q_id} 0 {doc_id} {score}\")\n\n                        if (\n                            i != len(self.qrels.keys()) - 1\n                            or j != len(self.qrels[q_id].keys()) - 1\n                        ):\n                            f.write(\"\\n\")\n\n    @staticmethod\n    def from_dict(d: Dict[str, Dict[str, int]]):\n        \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels.\n\n        Args:\n            d (Dict[str, Dict[str, int]]): Qrels as Python dictionary\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        # Query IDs\n        q_ids = list(d.keys())\n        q_ids = TypedList(q_ids)\n\n        # Doc IDs\n        doc_ids = [list(doc.keys()) for doc in d.values()]\n        max_len = max(len(y) for x in doc_ids for y in x)\n        dtype = f\"&lt;U{max_len}\"\n        doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n        # Scores\n        scores = [list(doc.values()) for doc in d.values()]\n        scores = TypedList([np.array(x, dtype=int) for x in scores])\n\n        qrels = Qrels()\n        qrels.qrels = create_and_sort(q_ids, doc_ids, scores)\n        qrels.sorted = True\n\n        return qrels\n\n    @staticmethod\n    def from_file(path: str, kind: str = None):\n        \"\"\"Parse a qrels file into ranx.Qrels. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str): File path.\n            kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Load Qrels -----------------------------------------------------------\n        if kind == \"json\":\n            qrels = orjson.loads(open(path, \"rb\").read())\n        else:\n            qrels = defaultdict(dict)\n            with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n                for line in f:\n                    q_id, _, doc_id, rel = line.split()\n                    qrels[q_id][doc_id] = int(rel)\n\n        return Qrels.from_dict(qrels)\n\n    @staticmethod\n    def from_df(\n        df: pd.DataFrame,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n    ):\n        \"\"\"Convert a Pandas DataFrame to ranx.Qrels.\n\n        Args:\n            df (pandas.DataFrame): Qrels as Pandas DataFrame.\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        assert (\n            df[q_id_col].dtype == \"O\"\n        ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n        assert (\n            df[doc_id_col].dtype == \"O\"\n        ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n        assert (\n            df[score_col].dtype == np.int64\n        ), \"DataFrame scores column dtype must be `int`\"\n\n        qrels_dict = (\n            df.groupby(q_id_col)[[doc_id_col, score_col]]\n            .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n            .to_dict()\n        )\n\n        return Qrels.from_dict(qrels_dict)\n\n    @staticmethod\n    def from_parquet(\n        path: str,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n        pd_kwargs: Dict[str, Any] = None,\n    ):\n        \"\"\"Convert a Parquet file to ranx.Qrels.\n\n        Args:\n            path (str): File path.\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n            pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n        return Qrels.from_df(\n            df=pd.read_parquet(path, *pd_kwargs),\n            q_id_col=q_id_col,\n            doc_id_col=doc_id_col,\n            score_col=score_col,\n        )\n\n    @staticmethod\n    def from_ir_datasets(dataset_id: str):\n        \"\"\"Convert `ir-datasets` qrels into ranx.Qrels. It automatically downloads data if missing.\n        Args:\n            dataset_id (str): ID of the detaset in `ir-datasets`. `ir-datasets` catalog is available here: https://ir-datasets.com/index.html.\n        Returns:\n            Qrels: ranx.Qrels\n        \"\"\"\n        qrels = Qrels.from_dict(ir_datasets.load(dataset_id).qrels_dict())\n        qrels.name = dataset_id\n        return qrels\n\n    @property\n    def size(self):\n        return len(self.qrels)\n\n    def __getitem__(self, q_id):\n        return dict(self.qrels[q_id])\n\n    def __len__(self) -&gt; int:\n        return len(self.qrels)\n\n    def __repr__(self):\n        return self.qrels.__repr__()\n\n    def __str__(self):\n        return self.qrels.__str__()\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.add","title":"<code>add(q_id, doc_ids, scores)</code>","text":"<p>Add a query and its relevant documents with the associated relevance score judgment.</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_ids</code> <code>List[str]</code> <p>List of Document IDs</p> required <code>scores</code> <code>List[int]</code> <p>List of relevance score judgments</p> required Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def add(self, q_id: str, doc_ids: List[str], scores: List[int]):\n    \"\"\"Add a query and its relevant documents with the associated relevance score judgment.\n\n    Args:\n        q_id (str): Query ID\n        doc_ids (List[str]): List of Document IDs\n        scores (List[int]): List of relevance score judgments\n    \"\"\"\n    self.add_multi([q_id], [doc_ids], [scores])\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.add_multi","title":"<code>add_multi(q_ids, doc_ids, scores)</code>","text":"<p>Add multiple queries at once.</p> <p>Parameters:</p> Name Type Description Default <code>q_ids</code> <code>List[str]</code> <p>List of Query IDs</p> required <code>doc_ids</code> <code>List[List[str]]</code> <p>List of list of Document IDs</p> required <code>scores</code> <code>List[List[int]]</code> <p>List of list of relevance score judgments</p> required Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def add_multi(\n    self,\n    q_ids: List[str],\n    doc_ids: List[List[str]],\n    scores: List[List[int]],\n):\n    \"\"\"Add multiple queries at once.\n\n    Args:\n        q_ids (List[str]): List of Query IDs\n        doc_ids (List[List[str]]): List of list of Document IDs\n        scores (List[List[int]]): List of list of relevance score judgments\n    \"\"\"\n    q_ids = TypedList(q_ids)\n    doc_ids = TypedList([TypedList(x) for x in doc_ids])\n    scores = TypedList([TypedList(map(int, x)) for x in scores])\n\n    self.qrels = add_and_sort(self.qrels, q_ids, doc_ids, scores)\n    self.sorted = True\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.add_score","title":"<code>add_score(q_id, doc_id, score)</code>","text":"<p>Add a (doc_id, score) pair to a query (or, change its value if it already exists).</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_id</code> <code>str</code> <p>Document ID</p> required <code>score</code> <code>int</code> <p>Relevance score judgment</p> required Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def add_score(self, q_id: str, doc_id: str, score: int):\n    \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n    Args:\n        q_id (str): Query ID\n        doc_id (str): Document ID\n        score (int): Relevance score judgment\n    \"\"\"\n    if self.qrels.get(q_id) is None:\n        self.qrels[q_id] = TypedDict.empty(\n            key_type=types.unicode_type,\n            value_type=types.int64,\n        )\n    self.qrels[q_id][doc_id] = int(score)\n    self.sorted = False\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.from_df","title":"<code>from_df(df, q_id_col='q_id', doc_id_col='doc_id', score_col='score')</code>  <code>staticmethod</code>","text":"<p>Convert a Pandas DataFrame to ranx.Qrels.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Qrels as Pandas DataFrame.</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance score judgments column. Defaults to \"score\".</p> <code>'score'</code> <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_df(\n    df: pd.DataFrame,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n):\n    \"\"\"Convert a Pandas DataFrame to ranx.Qrels.\n\n    Args:\n        df (pandas.DataFrame): Qrels as Pandas DataFrame.\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    assert (\n        df[q_id_col].dtype == \"O\"\n    ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n    assert (\n        df[doc_id_col].dtype == \"O\"\n    ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n    assert (\n        df[score_col].dtype == np.int64\n    ), \"DataFrame scores column dtype must be `int`\"\n\n    qrels_dict = (\n        df.groupby(q_id_col)[[doc_id_col, score_col]]\n        .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n        .to_dict()\n    )\n\n    return Qrels.from_dict(qrels_dict)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.from_dict","title":"<code>from_dict(d)</code>  <code>staticmethod</code>","text":"<p>Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Dict[str, int]]</code> <p>Qrels as Python dictionary</p> required <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_dict(d: Dict[str, Dict[str, int]]):\n    \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Qrels.\n\n    Args:\n        d (Dict[str, Dict[str, int]]): Qrels as Python dictionary\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    # Query IDs\n    q_ids = list(d.keys())\n    q_ids = TypedList(q_ids)\n\n    # Doc IDs\n    doc_ids = [list(doc.keys()) for doc in d.values()]\n    max_len = max(len(y) for x in doc_ids for y in x)\n    dtype = f\"&lt;U{max_len}\"\n    doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n    # Scores\n    scores = [list(doc.values()) for doc in d.values()]\n    scores = TypedList([np.array(x, dtype=int) for x in scores])\n\n    qrels = Qrels()\n    qrels.qrels = create_and_sort(q_ids, doc_ids, scores)\n    qrels.sorted = True\n\n    return qrels\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.from_file","title":"<code>from_file(path, kind=None)</code>  <code>staticmethod</code>","text":"<p>Parse a qrels file into ranx.Qrels. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>kind</code> <code>str</code> <p>Kind of file to load, must be either \"json\" or \"trec\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_file(path: str, kind: str = None):\n    \"\"\"Parse a qrels file into ranx.Qrels. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str): File path.\n        kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Load Qrels -----------------------------------------------------------\n    if kind == \"json\":\n        qrels = orjson.loads(open(path, \"rb\").read())\n    else:\n        qrels = defaultdict(dict)\n        with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n            for line in f:\n                q_id, _, doc_id, rel = line.split()\n                qrels[q_id][doc_id] = int(rel)\n\n    return Qrels.from_dict(qrels)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.from_ir_datasets","title":"<code>from_ir_datasets(dataset_id)</code>  <code>staticmethod</code>","text":"<p>Convert <code>ir-datasets</code> qrels into ranx.Qrels. It automatically downloads data if missing. Args:     dataset_id (str): ID of the detaset in <code>ir-datasets</code>. <code>ir-datasets</code> catalog is available here: https://ir-datasets.com/index.html. Returns:     Qrels: ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_ir_datasets(dataset_id: str):\n    \"\"\"Convert `ir-datasets` qrels into ranx.Qrels. It automatically downloads data if missing.\n    Args:\n        dataset_id (str): ID of the detaset in `ir-datasets`. `ir-datasets` catalog is available here: https://ir-datasets.com/index.html.\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    qrels = Qrels.from_dict(ir_datasets.load(dataset_id).qrels_dict())\n    qrels.name = dataset_id\n    return qrels\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.from_parquet","title":"<code>from_parquet(path, q_id_col='q_id', doc_id_col='doc_id', score_col='score', pd_kwargs=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Parquet file to ranx.Qrels.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance score judgments column. Defaults to \"score\".</p> <code>'score'</code> <code>pd_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments to pass to <code>pandas.read_parquet</code> (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Qrels</code> <p>ranx.Qrels</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>@staticmethod\ndef from_parquet(\n    path: str,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n    pd_kwargs: Dict[str, Any] = None,\n):\n    \"\"\"Convert a Parquet file to ranx.Qrels.\n\n    Args:\n        path (str): File path.\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance score judgments column. Defaults to \"score\".\n        pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n\n    Returns:\n        Qrels: ranx.Qrels\n    \"\"\"\n    pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n    return Qrels.from_df(\n        df=pd.read_parquet(path, *pd_kwargs),\n        q_id_col=q_id_col,\n        doc_id_col=doc_id_col,\n        score_col=score_col,\n    )\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.get_doc_ids_and_scores","title":"<code>get_doc_ids_and_scores()</code>","text":"<p>Returns doc ids and relevance judgments.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def get_doc_ids_and_scores(self):\n    \"\"\"Returns doc ids and relevance judgments.\"\"\"\n    return list(self.qrels.values())\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.get_query_ids","title":"<code>get_query_ids()</code>","text":"<p>Returns query ids.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def get_query_ids(self):\n    \"\"\"Returns query ids.\"\"\"\n    return list(self.qrels.keys())\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.keys","title":"<code>keys()</code>","text":"<p>Returns query ids. Used internally.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def keys(self):\n    \"\"\"Returns query ids. Used internally.\"\"\"\n    return self.qrels.keys()\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.save","title":"<code>save(path='qrels.json', kind=None)</code>","text":"<p>Write <code>qrels</code> to <code>path</code> as JSON file, TREC qrels format, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Saving path. Defaults to \"qrels.json\".</p> <code>'qrels.json'</code> <code>kind</code> <code>str</code> <p>Kind of file to save, must be either \"json\" or \"trec\". If None, it will be automatically inferred from the filename extension.</p> <code>None</code> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def save(self, path: str = \"qrels.json\", kind: str = None) -&gt; None:\n    \"\"\"Write `qrels` to `path` as JSON file, TREC qrels format, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str, optional): Saving path. Defaults to \"qrels.json\".\n        kind (str, optional): Kind of file to save, must be either \"json\" or \"trec\". If None, it will be automatically inferred from the filename extension.\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Save Qrels -----------------------------------------------------------\n    if kind == \"json\":\n        with open(path, \"wb\") as f:\n            f.write(orjson.dumps(self.to_dict(), option=orjson.OPT_INDENT_2))\n    elif kind == \"parquet\":\n        self.to_dataframe().to_parquet(path, index=False)\n    else:\n        with open(path, \"w\") as f:\n            for i, q_id in enumerate(self.qrels.keys()):\n                for j, doc_id in enumerate(self.qrels[q_id].keys()):\n                    score = self.qrels[q_id][doc_id]\n                    f.write(f\"{q_id} 0 {doc_id} {score}\")\n\n                    if (\n                        i != len(self.qrels.keys()) - 1\n                        or j != len(self.qrels[q_id].keys()) - 1\n                    ):\n                        f.write(\"\\n\")\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.set_relevance_level","title":"<code>set_relevance_level(rel_lvl=1)</code>","text":"<p>Sets relevance level.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def set_relevance_level(self, rel_lvl: int = 1):\n    \"\"\"Sets relevance level.\"\"\"\n    self.qrels = _set_relevance_level(self.qrels, rel_lvl)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.sort","title":"<code>sort()</code>","text":"<p>Sort. Used internally.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def sort(self):\n    \"\"\"Sort. Used internally.\"\"\"\n    self.qrels = sort_dict_by_key(self.qrels)\n    self.qrels = sort_dict_of_dict_by_value(self.qrels)\n    self.sorted = True\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert Qrels to Pandas DataFrame with the following columns: <code>q_id</code>, <code>doc_id</code>, and <code>score</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: Qrels as Pandas DataFrame.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert Qrels to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n    Returns:\n        pandas.DataFrame: Qrels as Pandas DataFrame.\n    \"\"\"\n    data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n    for q_id in self.qrels:\n        for doc_id in self.qrels[q_id]:\n            data[\"q_id\"].append(q_id)\n            data[\"doc_id\"].append(doc_id)\n            data[\"score\"].append(self.qrels[q_id][doc_id])\n\n    return pd.DataFrame.from_dict(data)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert Qrels to Python dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, int]]</code> <p>Dict[str, Dict[str, int]]: Qrels as Python dictionary</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Dict[str, int]]:\n    \"\"\"Convert Qrels to Python dictionary.\n\n    Returns:\n        Dict[str, Dict[str, int]]: Qrels as Python dictionary\n    \"\"\"\n    d = defaultdict(dict)\n    for q_id in self.keys():\n        d[q_id] = dict(self[q_id])\n    return d\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Qrels.to_typed_list","title":"<code>to_typed_list()</code>","text":"<p>Convert Qrels to Numba Typed List. Used internally.</p> Source code in <code>ranx/data_structures/qrels.py</code> <pre><code>def to_typed_list(self):\n    \"\"\"Convert Qrels to Numba Typed List. Used internally.\"\"\"\n    if not self.sorted:\n        self.sort()\n    return to_typed_list(self.qrels)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run","title":"<code>Run</code>","text":"<p>             Bases: <code>object</code></p> <p><code>Run</code> stores the relevance scores estimated by the model under evaluation.&lt;\br&gt; The preferred way for creating a <code>Run</code> instance is converting a Python dictionary as follows:</p> <pre><code>run_dict = {\n    \"q_1\": {\n        \"d_1\": 1.5,\n        \"d_2\": 2.6,\n    },\n    \"q_2\": {\n        \"d_3\": 2.8,\n        \"d_2\": 1.2,\n        \"d_5\": 3.1,\n    },\n}\n\nrun = Run(run_dict, name=\"bm25\")\n\nrun = Run()  # Creates an empty Run with no name\n</code></pre> Source code in <code>ranx/data_structures/run.py</code> <pre><code>class Run(object):\n    \"\"\"`Run` stores the relevance scores estimated by the model under evaluation.&lt;\\br&gt;\n    The preferred way for creating a `Run` instance is converting a Python dictionary as follows:\n\n    ```python\n    run_dict = {\n        \"q_1\": {\n            \"d_1\": 1.5,\n            \"d_2\": 2.6,\n        },\n        \"q_2\": {\n            \"d_3\": 2.8,\n            \"d_2\": 1.2,\n            \"d_5\": 3.1,\n        },\n    }\n\n    run = Run(run_dict, name=\"bm25\")\n\n    run = Run()  # Creates an empty Run with no name\n    ```\n    \"\"\"\n\n    def __init__(self, run: Dict[str, Dict[str, float]] = None, name: str = None):\n        if run is None:\n            self.run = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.DictType(types.unicode_type, types.float64),\n            )\n            self.sorted = False\n        else:\n            # Query IDs\n            q_ids = list(run.keys())\n            q_ids = TypedList(q_ids)\n\n            # Doc IDs\n            doc_ids = [list(doc.keys()) for doc in run.values()]\n            max_len = max(len(y) for x in doc_ids for y in x)\n            dtype = f\"&lt;U{max_len}\"\n            doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n            # Scores\n            scores = [list(doc.values()) for doc in run.values()]\n            scores = TypedList([np.array(x, dtype=float) for x in scores])\n            self.run = create_and_sort(q_ids, doc_ids, scores)\n            self.sorted = True\n\n        self.name = name\n        self.metadata = {}\n        self.scores = defaultdict(dict)\n        self.mean_scores = {}\n        self.std_scores = {}\n\n    def keys(self):\n        \"\"\"Returns query ids. Used internally.\"\"\"\n        return self.run.keys()\n\n    def add_score(self, q_id: str, doc_id: str, score: int):\n        \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n        Args:\n            q_id (str): Query ID\n            doc_id (str): Document ID\n            score (int): Relevance score\n        \"\"\"\n        if self.run.get(q_id) is None:\n            self.run[q_id] = TypedDict.empty(\n                key_type=types.unicode_type,\n                value_type=types.float64,\n            )\n        self.run[q_id][doc_id] = float(score)\n        self.sorted = False\n\n    def add(self, q_id: str, doc_ids: List[str], scores: List[float]):\n        \"\"\"Add a query and its relevant documents with the associated relevance score.\n\n        Args:\n            q_id (str): Query ID\n            doc_ids (List[str]): List of Document IDs\n            scores (List[int]): List of relevance scores\n        \"\"\"\n        self.add_multi([q_id], [doc_ids], [scores])\n\n    def add_multi(\n        self,\n        q_ids: List[str],\n        doc_ids: List[List[str]],\n        scores: List[List[float]],\n    ):\n        \"\"\"Add multiple queries at once.\n\n        Args:\n            q_ids (List[str]): List of Query IDs\n            doc_ids (List[List[str]]): List of list of Document IDs\n            scores (List[List[int]]): List of list of relevance scores\n        \"\"\"\n        q_ids = TypedList(q_ids)\n        doc_ids = TypedList([TypedList(x) for x in doc_ids])\n        scores = TypedList([TypedList(map(float, x)) for x in scores])\n\n        self.run = add_and_sort(self.run, q_ids, doc_ids, scores)\n        self.sorted = True\n\n    def get_query_ids(self):\n        \"\"\"Returns query ids.\"\"\"\n        return list(self.run.keys())\n\n    def get_doc_ids_and_scores(self):\n        \"\"\"Returns doc ids and relevance scores.\"\"\"\n        return list(self.run.values())\n\n    # Sort in place\n    def sort(self):\n        \"\"\"Sort. Used internally.\"\"\"\n        self.run = sort_dict_by_key(self.run)\n        self.run = sort_dict_of_dict_by_value(self.run)\n        self.sorted = True\n\n    def make_comparable(self, qrels: Qrels):\n        \"\"\"Adds empty results for queries missing from the run and removes those not appearing in qrels.\"\"\"\n        # Adds empty results for missing queries\n        for q_id in qrels.qrels:\n            if q_id not in self.run:\n                self.run[q_id] = create_empty_results_dict()\n\n        # Remove results for additional queries\n        for q_id in self.run:\n            if q_id not in qrels.qrels:\n                del self.run[q_id]\n\n        self.sort()\n\n        return self\n\n    def to_typed_list(self):\n        \"\"\"Convert Run to Numba Typed List. Used internally.\"\"\"\n        if not self.sorted:\n            self.sort()\n        return to_typed_list(self.run)\n\n    def to_dict(self):\n        \"\"\"Convert Run to Python dictionary.\n\n        Returns:\n            Dict[str, Dict[str, int]]: Run as Python dictionary\n        \"\"\"\n        d = defaultdict(dict)\n        for q_id in self.keys():\n            d[q_id] = dict(self[q_id])\n        return d\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert Run to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n        Returns:\n            pandas.DataFrame: Run as Pandas DataFrame.\n        \"\"\"\n        data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n        for q_id in self.run:\n            for doc_id in self.run[q_id]:\n                data[\"q_id\"].append(q_id)\n                data[\"doc_id\"].append(doc_id)\n                data[\"score\"].append(self.run[q_id][doc_id])\n\n        return pd.DataFrame.from_dict(data)\n\n    def save(self, path: str = \"run.json\", kind: str = None):\n        \"\"\"Write `run` to `path` as JSON file, TREC run, LZ4 file, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", and \".lz4\" -&gt; \"lz4\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str, optional): Saving path. Defaults to \"run.json\".\n            kind (str, optional): Kind of file to save, must be either \"json\", \"trec\", or \"ranxhub\". If None, it will be automatically inferred from the filename extension.\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Save Run -------------------------------------------------------------\n        if not self.sorted:\n            self.sort()\n\n        if kind == \"json\":\n            save_json(self.to_dict(), path)\n        elif kind == \"lz4\":\n            save_lz4(self.to_dict(), path)\n        elif kind == \"parquet\":\n            self.to_dataframe().to_parquet(path, index=False)\n        else:\n            with open(path, \"w\") as f:\n                for i, q_id in enumerate(self.run.keys()):\n                    for rank, doc_id in enumerate(self.run[q_id].keys()):\n                        score = self.run[q_id][doc_id]\n                        f.write(f\"{q_id} Q0 {doc_id} {rank+1} {score} {self.name}\")\n\n                        if (\n                            i != len(self.run.keys()) - 1\n                            or rank != len(self.run[q_id].keys()) - 1\n                        ):\n                            f.write(\"\\n\")\n\n    @staticmethod\n    def from_dict(d: Dict[str, Dict[str, float]], name: str = None):\n        \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run.\n\n        Args:\n            d (Dict[str, Dict[str, int]]): Run as Python dictionary\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n\n        # Query IDs\n        q_ids = list(d.keys())\n        q_ids = TypedList(q_ids)\n\n        # Doc IDs\n        doc_ids = [list(doc.keys()) for doc in d.values()]\n        max_len = max(len(y) for x in doc_ids for y in x)\n        dtype = f\"&lt;U{max_len}\"\n        doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n        # Scores\n        scores = [list(doc.values()) for doc in d.values()]\n        scores = TypedList([np.array(x, dtype=float) for x in scores])\n\n        run = Run()\n        run.run = create_and_sort(q_ids, doc_ids, scores)\n        run.sorted = True\n        run.name = name\n\n        return run\n\n    @staticmethod\n    def from_file(path: str, kind: str = None, name: str = None):\n        \"\"\"Parse a run file into ranx.Run. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\", \".lz4\" -&gt; \"lz4\". Use the \"kind\" argument to override this behavior.\n\n        Args:\n            path (str): File path.\n            kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        # Infer file extension -------------------------------------------------\n        kind = get_file_kind(path, kind)\n\n        # Load Run -------------------------------------------------------------\n        if kind == \"json\":\n            run = load_json(path)\n        elif kind == \"lz4\":\n            run = load_lz4(path)\n        else:\n            run = defaultdict(dict)\n            with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n                for line in f:\n                    q_id, _, doc_id, _, rel, run_name = line.split()\n                    run[q_id][doc_id] = float(rel)\n                    if name is None:\n                        name = run_name\n\n        run = Run.from_dict(run, name)\n\n        return run\n\n    @staticmethod\n    def from_df(\n        df: pd.DataFrame,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n        name: str = None,\n    ):\n        \"\"\"Convert a Pandas DataFrame to ranx.Run.\n\n        Args:\n            df (pd.DataFrame): Run as Pandas DataFrame\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance scores column. Defaults to \"score\".\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        assert (\n            df[q_id_col].dtype == \"O\"\n        ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n        assert (\n            df[doc_id_col].dtype == \"O\"\n        ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n        assert (\n            df[score_col].dtype == np.float64\n        ), \"DataFrame scores column dtype must be `float`\"\n\n        run_py = (\n            df.groupby(q_id_col)[[doc_id_col, score_col]]\n            .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n            .to_dict()\n        )\n\n        return Run.from_dict(run_py, name)\n\n    @staticmethod\n    def from_parquet(\n        path: str,\n        q_id_col: str = \"q_id\",\n        doc_id_col: str = \"doc_id\",\n        score_col: str = \"score\",\n        pd_kwargs: Dict[str, Any] = None,\n        name: str = None,\n    ):\n        \"\"\"Convert a Parquet file to ranx.Run.\n\n        Args:\n            path (str): File path.\n            q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n            doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n            score_col (str, optional): Relevance scores column. Defaults to \"score\".\n            pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n            name (str, optional): Run name. Defaults to None.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n        return Run.from_df(\n            df=pd.read_parquet(path, *pd_kwargs),\n            q_id_col=q_id_col,\n            doc_id_col=doc_id_col,\n            score_col=score_col,\n            name=name,\n        )\n\n    @staticmethod\n    def from_ranxhub(id: str):\n        \"\"\"Download and load a ranx.Run from ranxhub.\n\n        Args:\n            path (str): Run ID.\n\n        Returns:\n            Run: ranx.Run\n        \"\"\"\n        content = download(id)\n\n        run = Run.from_dict(content[\"run\"])\n        run.name = content[\"metadata\"][\"run\"][\"name\"]\n        run.metadata = content[\"metadata\"]\n\n        return run\n\n    @property\n    def size(self):\n        return len(self.run)\n\n    def __getitem__(self, q_id):\n        return dict(self.run[q_id])\n\n    def __len__(self) -&gt; int:\n        return len(self.run)\n\n    def __repr__(self):\n        return self.run.__repr__()\n\n    def __str__(self):\n        return self.run.__str__()\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.add","title":"<code>add(q_id, doc_ids, scores)</code>","text":"<p>Add a query and its relevant documents with the associated relevance score.</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_ids</code> <code>List[str]</code> <p>List of Document IDs</p> required <code>scores</code> <code>List[int]</code> <p>List of relevance scores</p> required Source code in <code>ranx/data_structures/run.py</code> <pre><code>def add(self, q_id: str, doc_ids: List[str], scores: List[float]):\n    \"\"\"Add a query and its relevant documents with the associated relevance score.\n\n    Args:\n        q_id (str): Query ID\n        doc_ids (List[str]): List of Document IDs\n        scores (List[int]): List of relevance scores\n    \"\"\"\n    self.add_multi([q_id], [doc_ids], [scores])\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.add_multi","title":"<code>add_multi(q_ids, doc_ids, scores)</code>","text":"<p>Add multiple queries at once.</p> <p>Parameters:</p> Name Type Description Default <code>q_ids</code> <code>List[str]</code> <p>List of Query IDs</p> required <code>doc_ids</code> <code>List[List[str]]</code> <p>List of list of Document IDs</p> required <code>scores</code> <code>List[List[int]]</code> <p>List of list of relevance scores</p> required Source code in <code>ranx/data_structures/run.py</code> <pre><code>def add_multi(\n    self,\n    q_ids: List[str],\n    doc_ids: List[List[str]],\n    scores: List[List[float]],\n):\n    \"\"\"Add multiple queries at once.\n\n    Args:\n        q_ids (List[str]): List of Query IDs\n        doc_ids (List[List[str]]): List of list of Document IDs\n        scores (List[List[int]]): List of list of relevance scores\n    \"\"\"\n    q_ids = TypedList(q_ids)\n    doc_ids = TypedList([TypedList(x) for x in doc_ids])\n    scores = TypedList([TypedList(map(float, x)) for x in scores])\n\n    self.run = add_and_sort(self.run, q_ids, doc_ids, scores)\n    self.sorted = True\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.add_score","title":"<code>add_score(q_id, doc_id, score)</code>","text":"<p>Add a (doc_id, score) pair to a query (or, change its value if it already exists).</p> <p>Parameters:</p> Name Type Description Default <code>q_id</code> <code>str</code> <p>Query ID</p> required <code>doc_id</code> <code>str</code> <p>Document ID</p> required <code>score</code> <code>int</code> <p>Relevance score</p> required Source code in <code>ranx/data_structures/run.py</code> <pre><code>def add_score(self, q_id: str, doc_id: str, score: int):\n    \"\"\"Add a (doc_id, score) pair to a query (or, change its value if it already exists).\n\n    Args:\n        q_id (str): Query ID\n        doc_id (str): Document ID\n        score (int): Relevance score\n    \"\"\"\n    if self.run.get(q_id) is None:\n        self.run[q_id] = TypedDict.empty(\n            key_type=types.unicode_type,\n            value_type=types.float64,\n        )\n    self.run[q_id][doc_id] = float(score)\n    self.sorted = False\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.from_df","title":"<code>from_df(df, q_id_col='q_id', doc_id_col='doc_id', score_col='score', name=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Pandas DataFrame to ranx.Run.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Run as Pandas DataFrame</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance scores column. Defaults to \"score\".</p> <code>'score'</code> <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_df(\n    df: pd.DataFrame,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n    name: str = None,\n):\n    \"\"\"Convert a Pandas DataFrame to ranx.Run.\n\n    Args:\n        df (pd.DataFrame): Run as Pandas DataFrame\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance scores column. Defaults to \"score\".\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    assert (\n        df[q_id_col].dtype == \"O\"\n    ), \"DataFrame Query IDs column dtype must be `object` (string)\"\n    assert (\n        df[doc_id_col].dtype == \"O\"\n    ), \"DataFrame Document IDs column dtype must be `object` (string)\"\n    assert (\n        df[score_col].dtype == np.float64\n    ), \"DataFrame scores column dtype must be `float`\"\n\n    run_py = (\n        df.groupby(q_id_col)[[doc_id_col, score_col]]\n        .apply(lambda g: {x[0]: x[1] for x in g.values.tolist()})\n        .to_dict()\n    )\n\n    return Run.from_dict(run_py, name)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.from_dict","title":"<code>from_dict(d, name=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Dict[str, int]]</code> <p>Run as Python dictionary</p> required <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_dict(d: Dict[str, Dict[str, float]], name: str = None):\n    \"\"\"Convert a Python dictionary in form of {q_id: {doc_id: score}} to ranx.Run.\n\n    Args:\n        d (Dict[str, Dict[str, int]]): Run as Python dictionary\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n\n    # Query IDs\n    q_ids = list(d.keys())\n    q_ids = TypedList(q_ids)\n\n    # Doc IDs\n    doc_ids = [list(doc.keys()) for doc in d.values()]\n    max_len = max(len(y) for x in doc_ids for y in x)\n    dtype = f\"&lt;U{max_len}\"\n    doc_ids = TypedList([np.array(x, dtype=dtype) for x in doc_ids])\n\n    # Scores\n    scores = [list(doc.values()) for doc in d.values()]\n    scores = TypedList([np.array(x, dtype=float) for x in scores])\n\n    run = Run()\n    run.run = create_and_sort(q_ids, doc_ids, scores)\n    run.sorted = True\n    run.name = name\n\n    return run\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.from_file","title":"<code>from_file(path, kind=None, name=None)</code>  <code>staticmethod</code>","text":"<p>Parse a run file into ranx.Run. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\", \".lz4\" -&gt; \"lz4\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>kind</code> <code>str</code> <p>Kind of file to load, must be either \"json\" or \"trec\".</p> <code>None</code> <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_file(path: str, kind: str = None, name: str = None):\n    \"\"\"Parse a run file into ranx.Run. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", \".gz\" -&gt; \"gzipped trec\", \".lz4\" -&gt; \"lz4\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str): File path.\n        kind (str, optional): Kind of file to load, must be either \"json\" or \"trec\".\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Load Run -------------------------------------------------------------\n    if kind == \"json\":\n        run = load_json(path)\n    elif kind == \"lz4\":\n        run = load_lz4(path)\n    else:\n        run = defaultdict(dict)\n        with gzip.open(path, \"rt\") if kind == \"gz\" else open(path) as f:\n            for line in f:\n                q_id, _, doc_id, _, rel, run_name = line.split()\n                run[q_id][doc_id] = float(rel)\n                if name is None:\n                    name = run_name\n\n    run = Run.from_dict(run, name)\n\n    return run\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.from_parquet","title":"<code>from_parquet(path, q_id_col='q_id', doc_id_col='doc_id', score_col='score', pd_kwargs=None, name=None)</code>  <code>staticmethod</code>","text":"<p>Convert a Parquet file to ranx.Run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path.</p> required <code>q_id_col</code> <code>str</code> <p>Query IDs column. Defaults to \"q_id\".</p> <code>'q_id'</code> <code>doc_id_col</code> <code>str</code> <p>Document IDs column. Defaults to \"doc_id\".</p> <code>'doc_id'</code> <code>score_col</code> <code>str</code> <p>Relevance scores column. Defaults to \"score\".</p> <code>'score'</code> <code>pd_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments to pass to <code>pandas.read_parquet</code> (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Run name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_parquet(\n    path: str,\n    q_id_col: str = \"q_id\",\n    doc_id_col: str = \"doc_id\",\n    score_col: str = \"score\",\n    pd_kwargs: Dict[str, Any] = None,\n    name: str = None,\n):\n    \"\"\"Convert a Parquet file to ranx.Run.\n\n    Args:\n        path (str): File path.\n        q_id_col (str, optional): Query IDs column. Defaults to \"q_id\".\n        doc_id_col (str, optional): Document IDs column. Defaults to \"doc_id\".\n        score_col (str, optional): Relevance scores column. Defaults to \"score\".\n        pd_kwargs (Dict[str, Any], optional): Additional arguments to pass to `pandas.read_parquet` (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). Defaults to None.\n        name (str, optional): Run name. Defaults to None.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    pd_kwargs = {} if pd_kwargs is None else pd_kwargs\n\n    return Run.from_df(\n        df=pd.read_parquet(path, *pd_kwargs),\n        q_id_col=q_id_col,\n        doc_id_col=doc_id_col,\n        score_col=score_col,\n        name=name,\n    )\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.from_ranxhub","title":"<code>from_ranxhub(id)</code>  <code>staticmethod</code>","text":"<p>Download and load a ranx.Run from ranxhub.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Run ID.</p> required <p>Returns:</p> Name Type Description <code>Run</code> <p>ranx.Run</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>@staticmethod\ndef from_ranxhub(id: str):\n    \"\"\"Download and load a ranx.Run from ranxhub.\n\n    Args:\n        path (str): Run ID.\n\n    Returns:\n        Run: ranx.Run\n    \"\"\"\n    content = download(id)\n\n    run = Run.from_dict(content[\"run\"])\n    run.name = content[\"metadata\"][\"run\"][\"name\"]\n    run.metadata = content[\"metadata\"]\n\n    return run\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.get_doc_ids_and_scores","title":"<code>get_doc_ids_and_scores()</code>","text":"<p>Returns doc ids and relevance scores.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def get_doc_ids_and_scores(self):\n    \"\"\"Returns doc ids and relevance scores.\"\"\"\n    return list(self.run.values())\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.get_query_ids","title":"<code>get_query_ids()</code>","text":"<p>Returns query ids.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def get_query_ids(self):\n    \"\"\"Returns query ids.\"\"\"\n    return list(self.run.keys())\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.keys","title":"<code>keys()</code>","text":"<p>Returns query ids. Used internally.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def keys(self):\n    \"\"\"Returns query ids. Used internally.\"\"\"\n    return self.run.keys()\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.make_comparable","title":"<code>make_comparable(qrels)</code>","text":"<p>Adds empty results for queries missing from the run and removes those not appearing in qrels.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def make_comparable(self, qrels: Qrels):\n    \"\"\"Adds empty results for queries missing from the run and removes those not appearing in qrels.\"\"\"\n    # Adds empty results for missing queries\n    for q_id in qrels.qrels:\n        if q_id not in self.run:\n            self.run[q_id] = create_empty_results_dict()\n\n    # Remove results for additional queries\n    for q_id in self.run:\n        if q_id not in qrels.qrels:\n            del self.run[q_id]\n\n    self.sort()\n\n    return self\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.save","title":"<code>save(path='run.json', kind=None)</code>","text":"<p>Write <code>run</code> to <code>path</code> as JSON file, TREC run, LZ4 file, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", and \".lz4\" -&gt; \"lz4\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Saving path. Defaults to \"run.json\".</p> <code>'run.json'</code> <code>kind</code> <code>str</code> <p>Kind of file to save, must be either \"json\", \"trec\", or \"ranxhub\". If None, it will be automatically inferred from the filename extension.</p> <code>None</code> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def save(self, path: str = \"run.json\", kind: str = None):\n    \"\"\"Write `run` to `path` as JSON file, TREC run, LZ4 file, or Parquet file. File type is automatically inferred form the filename extension: \".json\" -&gt; \"json\", \".trec\" -&gt; \"trec\", \".txt\" -&gt; \"trec\", and \".lz4\" -&gt; \"lz4\", \".parq\" -&gt; \"parquet\", \".parquet\" -&gt; \"parquet\". Use the \"kind\" argument to override this behavior.\n\n    Args:\n        path (str, optional): Saving path. Defaults to \"run.json\".\n        kind (str, optional): Kind of file to save, must be either \"json\", \"trec\", or \"ranxhub\". If None, it will be automatically inferred from the filename extension.\n    \"\"\"\n    # Infer file extension -------------------------------------------------\n    kind = get_file_kind(path, kind)\n\n    # Save Run -------------------------------------------------------------\n    if not self.sorted:\n        self.sort()\n\n    if kind == \"json\":\n        save_json(self.to_dict(), path)\n    elif kind == \"lz4\":\n        save_lz4(self.to_dict(), path)\n    elif kind == \"parquet\":\n        self.to_dataframe().to_parquet(path, index=False)\n    else:\n        with open(path, \"w\") as f:\n            for i, q_id in enumerate(self.run.keys()):\n                for rank, doc_id in enumerate(self.run[q_id].keys()):\n                    score = self.run[q_id][doc_id]\n                    f.write(f\"{q_id} Q0 {doc_id} {rank+1} {score} {self.name}\")\n\n                    if (\n                        i != len(self.run.keys()) - 1\n                        or rank != len(self.run[q_id].keys()) - 1\n                    ):\n                        f.write(\"\\n\")\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.sort","title":"<code>sort()</code>","text":"<p>Sort. Used internally.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def sort(self):\n    \"\"\"Sort. Used internally.\"\"\"\n    self.run = sort_dict_by_key(self.run)\n    self.run = sort_dict_of_dict_by_value(self.run)\n    self.sorted = True\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert Run to Pandas DataFrame with the following columns: <code>q_id</code>, <code>doc_id</code>, and <code>score</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: Run as Pandas DataFrame.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert Run to Pandas DataFrame with the following columns: `q_id`, `doc_id`, and `score`.\n\n    Returns:\n        pandas.DataFrame: Run as Pandas DataFrame.\n    \"\"\"\n    data = {\"q_id\": [], \"doc_id\": [], \"score\": []}\n\n    for q_id in self.run:\n        for doc_id in self.run[q_id]:\n            data[\"q_id\"].append(q_id)\n            data[\"doc_id\"].append(doc_id)\n            data[\"score\"].append(self.run[q_id][doc_id])\n\n    return pd.DataFrame.from_dict(data)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert Run to Python dictionary.</p> <p>Returns:</p> Type Description <p>Dict[str, Dict[str, int]]: Run as Python dictionary</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def to_dict(self):\n    \"\"\"Convert Run to Python dictionary.\n\n    Returns:\n        Dict[str, Dict[str, int]]: Run as Python dictionary\n    \"\"\"\n    d = defaultdict(dict)\n    for q_id in self.keys():\n        d[q_id] = dict(self[q_id])\n    return d\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.Run.to_typed_list","title":"<code>to_typed_list()</code>","text":"<p>Convert Run to Numba Typed List. Used internally.</p> Source code in <code>ranx/data_structures/run.py</code> <pre><code>def to_typed_list(self):\n    \"\"\"Convert Run to Numba Typed List. Used internally.\"\"\"\n    if not self.sorted:\n        self.sort()\n    return to_typed_list(self.run)\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.evaluate","title":"<code>evaluate(qrels, run, metrics, return_mean=True, return_std=False, threads=0, save_results_in_run=True, make_comparable=False)</code>","text":"<p>Compute the performance scores for the provided <code>qrels</code> and <code>run</code> for all the specified metrics.</p> <p>Usage examples:</p> <p>from ranx import evaluate</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--compute-score-for-a-single-metric","title":"Compute score for a single metric","text":"<p>evaluate(qrels, run, \"ndcg@5\")</p> <p>0.7861</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--compute-scores-for-multiple-metrics-at-once","title":"Compute scores for multiple metrics at once","text":"<p>evaluate(qrels, run, [\"map@5\", \"mrr\"])</p> <p>{\"map@5\": 0.6416, \"mrr\": 0.75}</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--computed-metric-scores-are-saved-in-the-run-object","title":"Computed metric scores are saved in the Run object","text":"<p>run.mean_scores</p> <p>{\"ndcg@5\": 0.7861, \"map@5\": 0.6416, \"mrr\": 0.75}</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--access-scores-for-each-query","title":"Access scores for each query","text":"<p>dict(run.scores)</p> <p>{ ...     \"ndcg@5\": {\"q_1\": 0.9430, \"q_2\": 0.6292}, ...     \"map@5\": {\"q_1\": 0.8333, \"q_2\": 0.4500}, ...     \"mrr\": {\"q_1\": 1.0000, \"q_2\": 0.5000}, ... } Args:     qrels (Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Qrels.     run (Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Run.     metrics (Union[List[str], str]): Metrics or list of metric to compute.     return_mean (bool, optional): Whether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True.     threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.     save_results_in_run (bool, optional): Save metric scores for each query in the input <code>run</code>. Defaults to True.     make_comparable (bool, optional): Adds empty results for queries missing from the run and removes those not appearing in qrels. Defaults to False.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, float], float]</code> <p>Union[Dict[str, float], float]: Results.</p> Source code in <code>ranx/meta/evaluate.py</code> <pre><code>def evaluate(\n    qrels: Union[\n        Qrels,\n        Dict[str, Dict[str, Number]],\n        nb.typed.typedlist.List,\n        np.ndarray,\n    ],\n    run: Union[\n        Run,\n        Dict[str, Dict[str, Number]],\n        nb.typed.typedlist.List,\n        np.ndarray,\n    ],\n    metrics: Union[List[str], str],\n    return_mean: bool = True,\n    return_std: bool = False,\n    threads: int = 0,\n    save_results_in_run: bool = True,\n    make_comparable: bool = False,\n) -&gt; Union[Dict[str, float], float]:\n    \"\"\"Compute the performance scores for the provided `qrels` and `run` for all the specified metrics.\n\n    Usage examples:\n\n    from ranx import evaluate\n\n    # Compute score for a single metric\n    evaluate(qrels, run, \"ndcg@5\")\n    &gt;&gt;&gt; 0.7861\n\n    # Compute scores for multiple metrics at once\n    evaluate(qrels, run, [\"map@5\", \"mrr\"])\n    &gt;&gt;&gt; {\"map@5\": 0.6416, \"mrr\": 0.75}\n\n    # Computed metric scores are saved in the Run object\n    run.mean_scores\n    &gt;&gt;&gt; {\"ndcg@5\": 0.7861, \"map@5\": 0.6416, \"mrr\": 0.75}\n\n    # Access scores for each query\n    dict(run.scores)\n    &gt;&gt;&gt; {\n    ...     \"ndcg@5\": {\"q_1\": 0.9430, \"q_2\": 0.6292},\n    ...     \"map@5\": {\"q_1\": 0.8333, \"q_2\": 0.4500},\n    ...     \"mrr\": {\"q_1\": 1.0000, \"q_2\": 0.5000},\n    ... }\n    Args:\n        qrels (Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Qrels.\n        run (Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Run.\n        metrics (Union[List[str], str]): Metrics or list of metric to compute.\n        return_mean (bool, optional): Whether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True.\n        threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.\n        save_results_in_run (bool, optional): Save metric scores for each query in the input `run`. Defaults to True.\n        make_comparable (bool, optional): Adds empty results for queries missing from the run and removes those not appearing in qrels. Defaults to False.\n\n    Returns:\n        Union[Dict[str, float], float]: Results.\n    \"\"\"\n\n    if len(qrels) &lt; 10:\n        set_num_threads(1)\n    elif threads != 0:\n        set_num_threads(threads)\n\n    if not return_mean:\n        return_std = False\n\n    if make_comparable and type(qrels) == Qrels and type(run) == Run:\n        run = run.make_comparable(qrels)\n\n    if type(qrels) in [Qrels, dict] and type(run) in [Run, dict]:\n        check_keys(qrels, run)\n\n    _qrels = convert_qrels(qrels)\n    _run = convert_run(run)\n    metrics = format_metrics(metrics)\n    assert all(isinstance(m, str) for m in metrics), \"Metrics error\"\n\n    # Compute metrics ----------------------------------------------------------\n    metric_scores_dict = {}\n    for metric in metrics:\n        m, k, rel_lvl = extract_metric_and_params(metric)\n        metric_scores_dict[metric] = metric_switch(m)(_qrels, _run, k, rel_lvl)\n\n    # Save results in Run ------------------------------------------------------\n    if type(run) == Run and save_results_in_run:\n        for m, scores in metric_scores_dict.items():\n            run.mean_scores[m] = np.mean(scores)\n            if return_std:\n                run.std_scores[m] = np.std(scores)\n            for i, q_id in enumerate(run.get_query_ids()):\n                run.scores[m][q_id] = scores[i]\n\n    # Prepare output -----------------------------------------------------------\n    if return_mean:\n        for m, scores in metric_scores_dict.items():\n            if return_std:\n                metric_scores_dict[m] = {\n                    \"mean\": np.mean(scores),\n                    \"std\": np.std(scores),\n                }\n\n            else:\n                metric_scores_dict[m] = np.mean(scores)\n\n    return metric_scores_dict[m] if len(metrics) == 1 else metric_scores_dict\n</code></pre>"},{"location":"evaluate/#ranx.evaluate.python_dict_to_typed_list","title":"<code>python_dict_to_typed_list(x, sort=True)</code>","text":"<p>Converts a nested Python Dictionary to Numba Typed List to be used with ranx's metrics with no effort.</p> <p>Note: Doc IDs will be hashed.</p> Source code in <code>ranx/utils.py</code> <pre><code>def python_dict_to_typed_list(x: Dict[str, Dict[str, Number]], sort: bool = True):\n    \"\"\"Converts a nested Python Dictionary to Numba Typed List to be used with ranx's metrics with no effort.\n\n    Note: Doc IDs will be hashed.\n    \"\"\"\n    out = TypedList(\n        [\n            np.array(\n                [[hash(doc_id), score] for doc_id, score in doc.items()],\n                dtype=np.float64,\n            )\n            for doc in x.values()\n        ]\n    )\n\n    if sort:\n        out = descending_sort_parallel(out)\n\n    return out\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-ranx-suited-for-evaluating-classification-tasks","title":"Is <code>ranx</code> suited for evaluating classification tasks?","text":"<p>No, it's not. <code>ranx</code> is meant for ranking tasks. Although some metrics are commonly used for evaluation of both tasks (e.g., <code>precision</code> and <code>recall</code>) the relevance scores stored in <code>runs</code> should not be confused with the predicted class labels of a classification task. Relevance scores are used by <code>ranx</code> to sort results before computing the metrics, regardless of their actual values.</p>"},{"location":"faq/#are-zero-and-negative-scored-results-filtered-out-by-ranx","title":"Are zero and negative scored results filtered out by <code>ranx</code>?","text":"<p>Zero and negative scored results are NOT filtered out by <code>ranx</code>. Relevance scores are used only for sorting and there is no constraint on the values produce by a ranking models, although some of them only outputs positive values. Therefore, if you think that zero and negative scored results should be filtered out, you should do it before passing the <code>runs</code> to <code>ranx</code>.</p>"},{"location":"fusion/","title":"Fusion","text":""},{"location":"fusion/#fuse","title":"Fuse","text":"<p><code>ranx</code> provides several fusion algorithms, all of which can be accessed through a single function in the same fashion as <code>evaluate</code>.</p> <p>Usage example: <pre><code>from ranx import fuse\n\ncombined_run = fuse(\n    runs=[run_1, run_2],  # A list of Run instances to fuse\n    norm=\"min-max\",       # The normalization strategy to apply before fusion\n    method=\"max\",         # The fusion algorithm to use \n)\n</code></pre></p>"},{"location":"fusion/#optimize-fusion","title":"Optimize Fusion","text":"<p>As many fusion algorithms require a training or optimization step, <code>ranx</code> provides a function to optimize all of those algorithms. For algorithms requiring hyper-parameter optimization, <code>ranx</code> automatically evaluates pre-defined configurations via grid search. In those cases, <code>ranx</code> shows a progress bar.</p> <p>Usage example: <pre><code>from ranx import fuse, optimize_fusion\n\nbest_params = optimize_fusion(\n    qrels=train_qrels,\n    runs=[train_run_1, train_run_2, train_run_3],\n    norm=\"min-max\",\n    method=\"wsum\",\n    metric=\"ndcg@100\",  # The metric to maximize during optimization\n)\n\ncombined_test_run = fuse(\n    runs=[test_run_1, test_run_2, test_run_3],  \n    norm=\"min-max\",       \n    method=\"wsum\",        \n    params=best_params,\n)\n</code></pre></p>"},{"location":"fusion/#supported-fusion-algorithms","title":"Supported fusion algorithms","text":"<p><code>ranx</code> supports the following fusion algorithms:</p> Algorithm Alias Optim. Algorithm Alias Optim. CombMIN min No CombMAX max No CombMED med No CombSUM sum No CombANZ anz No CombMNZ mnz No CombGMNZ gmnz Yes ISR isr No Log_ISR log_isr No LogN_ISR logn_isr Yes Reciprocal Rank Fusion (RRF) rrf Yes PosFuse posfuse Yes ProbFuse probfuse Yes SegFuse segfuse Yes SlideFuse slidefuse Yes MAPFuse mapfuse Yes BordaFuse bordafuse No Weighted BordaFuse w_bordafuse Yes Condorcet condorcet No Weighted Condorcet w_condorcet Yes BayesFuse bayesfuse Yes Mixed mixed Yes WMNZ wmnz Yes Weighted Sum wsum Yes Rank-Biased Centroids (RBC) rbc Yes"},{"location":"fusion/#bayesfuse","title":"BayesFuse","text":"<p>Computes BayesFuse as proposed by Aslam et al..  </p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/AslamM01,\n    author    = {Javed A. Aslam and\n                Mark H. Montague},\n    editor    = {W. Bruce Croft and\n                David J. Harper and\n                Donald H. Kraft and\n                Justin Zobel},\n    title     = {Models for Metasearch},\n    booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, September\n                9-13, 2001, New Orleans, Louisiana, {USA}},\n    pages     = {275--284},\n    publisher = {{ACM}},\n    year      = {2001},\n    url       = {https://doi.org/10.1145/383952.384007},\n    doi       = {10.1145/383952.384007},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/AslamM01.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#bordafuse","title":"BordaFuse","text":"<p>Computes BordaFuse as proposed by Aslam et al..  </p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/AslamM01,\n    author    = {Javed A. Aslam and\n                Mark H. Montague},\n    editor    = {W. Bruce Croft and\n                David J. Harper and\n                Donald H. Kraft and\n                Justin Zobel},\n    title     = {Models for Metasearch},\n    booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, September\n                9-13, 2001, New Orleans, Louisiana, {USA}},\n    pages     = {275--284},\n    publisher = {{ACM}},\n    year      = {2001},\n    url       = {https://doi.org/10.1145/383952.384007},\n    doi       = {10.1145/383952.384007},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/AslamM01.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#combanz","title":"CombANZ","text":"<p>Computes CombANZ as proposed by Fox et al..  </p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combgmnz","title":"CombGMNZ","text":"<p>Computes CombGMNZ as proposed by Joon Ho Lee.</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/Lee97,\n    author    = {Joon Ho Lee},\n    title     = {Analyses of Multiple Evidence Combination},\n    booktitle = {{SIGIR}},\n    pages     = {267--276},\n    publisher = {{ACM}},\n    year      = {1997}\n}\n</code></pre> Optimization Parameter Default Value min_gamma 0.01 max_gamma 1.0 step 0.01"},{"location":"fusion/#combmax","title":"CombMAX","text":"<p>Computes CombMAX as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combmed","title":"CombMED","text":"<p>Computes CombMED as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combmin","title":"CombMIN","text":"<p>Computes CombMIN as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combmnz","title":"CombMNZ","text":"<p>Computes CombMNZ as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combsum","title":"CombSUM","text":"<p>Computes CombSUM as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#condorcet","title":"Condorcet","text":"<p>Computes Condorcet as proposed by Montague et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/MontagueA02,\n    author    = {Mark H. Montague and\n                Javed A. Aslam},\n    title     = {Condorcet fusion for improved retrieval},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {538--548},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584881},\n    doi       = {10.1145/584792.584881},\n    timestamp = {Tue, 06 Nov 2018 16:57:50 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/MontagueA02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#isr","title":"ISR","text":"<p>Computes ISR as proposed by Mour\u00e3o et al..</p> BibTeX <pre><code>@article{DBLP:journals/cmig/MouraoMM15,\n    author    = {Andr{\\'{e}} Mour{\\~{a}}o and\n                Fl{\\'{a}}vio Martins and\n                Jo{\\~{a}}o Magalh{\\~{a}}es},\n    title     = {Multimodal medical information retrieval with unsupervised rank fusion},\n    journal   = {Comput. Medical Imaging Graph.},\n    volume    = {39},\n    pages     = {35--45},\n    year      = {2015},\n    url       = {https://doi.org/10.1016/j.compmedimag.2014.05.006},\n    doi       = {10.1016/j.compmedimag.2014.05.006},\n    timestamp = {Thu, 14 May 2020 10:17:16 +0200},\n    biburl    = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#log_isr","title":"Log_ISR","text":"<p>Computes Log_ISR as proposed by Mour\u00e3o et al..</p> BibTeX <pre><code>@article{DBLP:journals/cmig/MouraoMM15,\n    author    = {Andr{\\'{e}} Mour{\\~{a}}o and\n                Fl{\\'{a}}vio Martins and\n                Jo{\\~{a}}o Magalh{\\~{a}}es},\n    title     = {Multimodal medical information retrieval with unsupervised rank fusion},\n    journal   = {Comput. Medical Imaging Graph.},\n    volume    = {39},\n    pages     = {35--45},\n    year      = {2015},\n    url       = {https://doi.org/10.1016/j.compmedimag.2014.05.006},\n    doi       = {10.1016/j.compmedimag.2014.05.006},\n    timestamp = {Thu, 14 May 2020 10:17:16 +0200},\n    biburl    = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#logn_isr","title":"LogN_ISR","text":"<p>Computes Log_ISR as proposed by Mour\u00e3o et al..</p> BibTeX <pre><code>@article{DBLP:journals/cmig/MouraoMM15,\n    author    = {Andr{\\'{e}} Mour{\\~{a}}o and\n                Fl{\\'{a}}vio Martins and\n                Jo{\\~{a}}o Magalh{\\~{a}}es},\n    title     = {Multimodal medical information retrieval with unsupervised rank fusion},\n    journal   = {Comput. Medical Imaging Graph.},\n    volume    = {39},\n    pages     = {35--45},\n    year      = {2015},\n    url       = {https://doi.org/10.1016/j.compmedimag.2014.05.006},\n    doi       = {10.1016/j.compmedimag.2014.05.006},\n    timestamp = {Thu, 14 May 2020 10:17:16 +0200},\n    biburl    = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_sigma 0.01 max_sigma 1.0 step 0.01"},{"location":"fusion/#mapfuse","title":"MAPFuse","text":"<p>Computes MAPFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/LillisZTCLD10,\n    author    = {David Lillis and\n                Lusheng Zhang and\n                Fergus Toolan and\n                Rem W. Collier and\n                David Leonard and\n                John Dunnion},\n    editor    = {Fabio Crestani and\n                St{\\'{e}}phane Marchand{-}Maillet and\n                Hsin{-}Hsi Chen and\n                Efthimis N. Efthimiadis and\n                Jacques Savoy},\n    title     = {Estimating probabilities for effective data fusion},\n    booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research\n                and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland,\n                July 19-23, 2010},\n    pages     = {347--354},\n    publisher = {{ACM}},\n    year      = {2010},\n    url       = {https://doi.org/10.1145/1835449.1835508},\n    doi       = {10.1145/1835449.1835508},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#mixed","title":"Mixed","text":"<p>Computes Mixed as proposed by Wu et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/WuC02,\n    author    = {Shengli Wu and\n                Fabio Crestani},\n    title     = {Data fusion with estimated weights},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {648--651},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584908},\n    doi       = {10.1145/584792.584908},\n    timestamp = {Tue, 06 Nov 2018 16:57:40 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/WuC02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#posfuse","title":"PosFuse","text":"<p>Computes PosFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/LillisZTCLD10,\n    author    = {David Lillis and\n                Lusheng Zhang and\n                Fergus Toolan and\n                Rem W. Collier and\n                David Leonard and\n                John Dunnion},\n    editor    = {Fabio Crestani and\n                St{\\'{e}}phane Marchand{-}Maillet and\n                Hsin{-}Hsi Chen and\n                Efthimis N. Efthimiadis and\n                Jacques Savoy},\n    title     = {Estimating probabilities for effective data fusion},\n    booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research\n                and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland,\n                July 19-23, 2010},\n    pages     = {347--354},\n    publisher = {{ACM}},\n    year      = {2010},\n    url       = {https://doi.org/10.1145/1835449.1835508},\n    doi       = {10.1145/1835449.1835508},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#probfuse","title":"ProbFuse","text":"<p>Computes ProbFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/LillisTCD06,\n    author    = {David Lillis and\n                Fergus Toolan and\n                Rem W. Collier and\n                John Dunnion},\n    editor    = {Efthimis N. Efthimiadis and\n                Susan T. Dumais and\n                David Hawking and\n                Kalervo J{\\\"{a}}rvelin},\n    title     = {ProbFuse: a probabilistic approach to data fusion},\n    booktitle = {{SIGIR} 2006: Proceedings of the 29th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, Seattle,\n                Washington, USA, August 6-11, 2006},\n    pages     = {139--146},\n    publisher = {{ACM}},\n    year      = {2006},\n    url       = {https://doi.org/10.1145/1148170.1148197},\n    doi       = {10.1145/1148170.1148197},\n    timestamp = {Wed, 14 Nov 2018 10:58:10 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/LillisTCD06.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_n_segments 1 max_n_segments 100"},{"location":"fusion/#rank-biased-centroids-rbc","title":"Rank-Biased Centroids (RBC)","text":"<p>Computes Rank-Biased Centroid (RBC) as proposed by Bailey et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/BaileyMST17,\n    author    = {Peter Bailey and\n                Alistair Moffat and\n                Falk Scholer and\n                Paul Thomas},\n    editor    = {Noriko Kando and\n                Tetsuya Sakai and\n                Hideo Joho and\n                Hang Li and\n                Arjen P. de Vries and\n                Ryen W. White},\n    title     = {Retrieval Consistency in the Presence of Query Variations},\n    booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on\n                Research and Development in Information Retrieval, Shinjuku, Tokyo,\n                Japan, August 7-11, 2017},\n    pages     = {395--404},\n    publisher = {{ACM}},\n    year      = {2017},\n    url       = {https://doi.org/10.1145/3077136.3080839},\n    doi       = {10.1145/3077136.3080839},\n    timestamp = {Wed, 25 Sep 2019 16:43:14 +0200},\n    biburl    = {https://dblp.org/rec/conf/sigir/BaileyMST17.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_phi 0.01 max_phi 1.0 step 0.01"},{"location":"fusion/#reciprocal-rank-fusion-rrf","title":"Reciprocal Rank Fusion (RRF)","text":"<p>Computes Reciprocal Rank Fusion as proposed by Cormack et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/CormackCB09,\n    author    = {Gordon V. Cormack and\n                Charles L. A. Clarke and\n                Stefan B{\\\"{u}}ttcher},\n    title     = {Reciprocal rank fusion outperforms condorcet and individual rank learning\n                methods},\n    booktitle = {{SIGIR}},\n    pages     = {758--759},\n    publisher = {{ACM}},\n    year      = {2009}\n}\n</code></pre> Optimization Parameter Default Value min_k 10 max_k 100 step 10"},{"location":"fusion/#segfuse","title":"SegFuse","text":"<p>Computes SegFuse as proposed by Shokouhi.</p> BibTeX <pre><code>@inproceedings{DBLP:conf/ecir/Shokouhi07a,\n    author    = {Milad Shokouhi},\n    editor    = {Giambattista Amati and\n                Claudio Carpineto and\n                Giovanni Romano},\n    title     = {Segmentation of Search Engine Results for Effective Data-Fusion},\n    booktitle = {Advances in Information Retrieval, 29th European Conference on {IR}\n                Research, {ECIR} 2007, Rome, Italy, April 2-5, 2007, Proceedings},\n    series    = {Lecture Notes in Computer Science},\n    volume    = {4425},\n    pages     = {185--197},\n    publisher = {Springer},\n    year      = {2007},\n    url       = {https://doi.org/10.1007/978-3-540-71496-5\\_19},\n    doi       = {10.1007/978-3-540-71496-5\\_19},\n    timestamp = {Tue, 14 May 2019 10:00:37 +0200},\n    biburl    = {https://dblp.org/rec/conf/ecir/Shokouhi07a.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#slidefuse","title":"SlideFuse","text":"<p>Computes SlideFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/ecir/LillisTCD08,\n    author    = {David Lillis and\n                Fergus Toolan and\n                Rem W. Collier and\n                John Dunnion},\n    editor    = {Craig Macdonald and\n                Iadh Ounis and\n                Vassilis Plachouras and\n                Ian Ruthven and\n                Ryen W. White},\n    title     = {Extending Probabilistic Data Fusion Using Sliding Windows},\n    booktitle = {Advances in Information Retrieval , 30th European Conference on {IR}\n                Research, {ECIR} 2008, Glasgow, UK, March 30-April 3, 2008. Proceedings},\n    series    = {Lecture Notes in Computer Science},\n    volume    = {4956},\n    pages     = {358--369},\n    publisher = {Springer},\n    year      = {2008},\n    url       = {https://doi.org/10.1007/978-3-540-78646-7\\_33},\n    doi       = {10.1007/978-3-540-78646-7\\_33},\n    timestamp = {Sun, 25 Oct 2020 22:33:08 +0100},\n    biburl    = {https://dblp.org/rec/conf/ecir/LillisTCD08.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_w 1 max_w 100"},{"location":"fusion/#weighted-bordafuse","title":"Weighted BordaFuse","text":"<p>Computes Weighted BordaFuse as proposed by Aslam et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/AslamM01,\n    author    = {Javed A. Aslam and\n                Mark H. Montague},\n    editor    = {W. Bruce Croft and\n                David J. Harper and\n                Donald H. Kraft and\n                Justin Zobel},\n    title     = {Models for Metasearch},\n    booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, September\n                9-13, 2001, New Orleans, Louisiana, {USA}},\n    pages     = {275--284},\n    publisher = {{ACM}},\n    year      = {2001},\n    url       = {https://doi.org/10.1145/383952.384007},\n    doi       = {10.1145/383952.384007},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/AslamM01.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#weighted-condorcet","title":"Weighted Condorcet","text":"<p>Computes Weighted Condorcet as proposed by Montague et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/MontagueA02,\n    author    = {Mark H. Montague and\n                Javed A. Aslam},\n    title     = {Condorcet fusion for improved retrieval},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {538--548},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584881},\n    doi       = {10.1145/584792.584881},\n    timestamp = {Tue, 06 Nov 2018 16:57:50 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/MontagueA02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#wmnz","title":"WMNZ","text":"<p>Computes Weighted MNZ as proposed by Wu et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/WuC02,\n    author    = {Shengli Wu and\n                Fabio Crestani},\n    title     = {Data fusion with estimated weights},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {648--651},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584908},\n    doi       = {10.1145/584792.584908},\n    timestamp = {Tue, 06 Nov 2018 16:57:40 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/WuC02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#weighted-sum","title":"Weighted Sum","text":"<p>Computes a weighted sum of the scores given to documents by a list of Runs.</p> Optimization Parameter Default Value step 0.1"},{"location":"metrics/","title":"Metrics","text":""},{"location":"metrics/#aliases","title":"Aliases","text":"<p>Aliases to use with <code>ranx.evaluate</code> and <code>ranx.compare</code>.</p> Metric Alias @k .p Hits hits Yes No Hit Rate / Success hit_rate Yes No Precision precision Yes No Recall recall Yes No F1 f1 Yes No R-Precision r_precision No No Bpref bpref No No Rank-biased Precision rbp No Yes Mean Reciprocal Rank mrr Yes No Mean Average Precision map Yes No DCG dcg Yes No DCG Burges dcg_burges Yes No NDCG ndcg Yes No NDCG Burges ndcg_burges Yes No"},{"location":"metrics/#hits","title":"Hits","text":"<p>Hits is the number of relevant documents retrieved.</p>"},{"location":"metrics/#hit-rate-success","title":"Hit Rate / Success","text":"<p>Hit Rate is the fraction of queries for which at least one relevant document is retrieved. Note: it is equivalent to <code>success</code> from trec_eval.</p>"},{"location":"metrics/#precision","title":"Precision","text":"<p>Precision is the proportion of the retrieved documents that are relevant.</p> \\[ \\operatorname{Precision}=\\frac{r}{n} \\] <p>where,</p> <ul> <li>\\(r\\) is the number of retrieved relevant documents;</li> <li>\\(n\\) is the number of retrieved documents.</li> </ul>"},{"location":"metrics/#recall","title":"Recall","text":"<p>Recall is the ratio between the retrieved documents that are relevant and the total number of relevant documents.</p> \\[ \\operatorname{Recall}=\\frac{r}{R} \\] <p>where,</p> <ul> <li>\\(r\\) is the number of retrieved relevant documents;</li> <li>\\(R\\) is the total number of relevant documents.</li> </ul>"},{"location":"metrics/#f1","title":"F1","text":"<p>F1 is the harmonic mean of Precision and Recall.</p> \\[ \\operatorname{F1} = 2 \\times \\frac{\\operatorname{Precision} \\times \\operatorname{Recall}}{\\operatorname{Precision} + \\operatorname{Recall}} \\]"},{"location":"metrics/#r-precision","title":"R-Precision","text":"<p>For a given query \\(Q\\), R-Precision is the precision at \\(R\\), where \\(R\\) is the number of relevant documents for \\(Q\\). In other words, if there are \\(r\\) relevant documents among the top-\\(R\\) retrieved documents, then R-precision is:</p> \\[ \\operatorname{R-Precision} = \\frac{r}{R} \\]"},{"location":"metrics/#bpref","title":"Bpref","text":"<p>Bpref is designed for situations where relevance judgments are known to be incomplete. It is defined as:</p> \\[ \\operatorname{bpref}=\\frac{1}{R}\\sum_r{1 - \\frac{|\\text{$n$ ranked higher than $r$}|}{R}} \\] <p>where,</p> <ul> <li>\\(r\\) is a relevant document;</li> <li>\\(n\\) is a member of the first R judged nonrelevant documents as retrieved by the system;</li> <li>\\(R\\) is the number of relevant documents.</li> </ul>"},{"location":"metrics/#rank-biased-precision","title":"Rank-biased Precision","text":"<p>Compute Rank-biased Precision (RBP).</p> <p>It is defined as:</p> \\[ \\operatorname{RBP} = (1 - p) \\cdot \\sum_{i=1}^{d}{r_i \\cdot p^{i - 1}} \\] <p>where,</p> <ul> <li>\\(p\\) is the persistence value;</li> <li>\\(r_i\\) is either 0 or 1, whether the \\(i\\)-th ranked document is non-relevant or relevant, respectively.</li> </ul>"},{"location":"metrics/#mean-reciprocal-rank","title":"(Mean) Reciprocal Rank","text":"<p>Reciprocal Rank is the multiplicative inverse of the rank of the first retrieved relevant document: 1 for first place, 1/2 for second place, 1/3 for third place, and so on. When averaged over many queries, it is usually called Mean Reciprocal Rank (MRR).</p> \\[ Reciprocal Rank = \\frac{1}{rank} \\] <p>where,</p> <ul> <li>\\(rank\\) is the position of the first retrieved relevant document.</li> </ul>"},{"location":"metrics/#mean-average-precision","title":"(Mean) Average Precision","text":"<p>Average Precision is the average of the Precision scores computed after each relevant document is retrieved. When averaged over many queries, it is usually called Mean Average Precision (MAP).</p> \\[ \\operatorname{Average Precision} = \\frac{\\sum_r \\operatorname{Precision}@r}{R} \\] <p>where,</p> <ul> <li>\\(r\\) is the position of a relevant document;</li> <li>\\(R\\) is the total number of relevant documents.</li> </ul>"},{"location":"metrics/#dcg","title":"DCG","text":"<p>Compute Discounted Cumulative Gain (DCG) as proposed by J\u00e4rvelin et al..</p> BibTeX <pre><code>@article{DBLP:journals/tois/JarvelinK02,\n    author    = {Kalervo J{\\\"{a}}rvelin and\n                Jaana Kek{\\\"{a}}l{\\\"{a}}inen},\n    title     = {Cumulated gain-based evaluation of {IR} techniques},\n    journal   = {{ACM} Trans. Inf. Syst.},\n    volume    = {20},\n    number    = {4},\n    pages     = {422--446},\n    year      = {2002}\n}\n</code></pre> \\[ \\operatorname{DCG} = \\frac{\\operatorname{rel}_i}{\\log_2(i+1)} \\] <p>where,</p> <ul> <li>\\(\\operatorname{rel}_i\\) is the relevance value of the result at position i.</li> </ul>"},{"location":"metrics/#dcg-burges","title":"DCG Burges","text":"<p>Compute Discounted Cumulative Gain (DCG) at k as proposed by Burges et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/icml/BurgesSRLDHH05,\n    author    = {Christopher J. C. Burges and\n                Tal Shaked and\n                Erin Renshaw and\n                Ari Lazier and\n                Matt Deeds and\n                Nicole Hamilton and\n                Gregory N. Hullender},\n    title     = {Learning to rank using gradient descent},\n    booktitle = {{ICML}},\n    series    = {{ACM} International Conference Proceeding Series},\n    volume    = {119},\n    pages     = {89--96},\n    publisher = {{ACM}},\n    year      = {2005}\n}\n</code></pre> \\[ \\operatorname{DCG} = \\frac{2^{\\operatorname{rel}_i-1}}{\\log_2(i+1)} \\] <p>where,</p> <ul> <li>\\(\\operatorname{rel}_i\\) is the relevance value of the result at position i.</li> </ul>"},{"location":"metrics/#ndcg","title":"NDCG","text":"<p>Compute Normalized Discounted Cumulative Gain (NDCG) as proposed by J\u00e4rvelin et al..</p> BibTeX <pre><code>@article{DBLP:journals/tois/JarvelinK02,\n    author    = {Kalervo J{\\\"{a}}rvelin and\n                Jaana Kek{\\\"{a}}l{\\\"{a}}inen},\n    title     = {Cumulated gain-based evaluation of {IR} techniques},\n    journal   = {{ACM} Trans. Inf. Syst.},\n    volume    = {20},\n    number    = {4},\n    pages     = {422--446},\n    year      = {2002}\n}\n</code></pre> \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] <p>where,</p> <ul> <li>\\(\\operatorname{DCG}\\) is Discounted Cumulative Gain;</li> <li>\\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possible DCG).</li> </ul>"},{"location":"metrics/#ndcg-burges","title":"NDCG Burges","text":"<p>Compute Normalized Discounted Cumulative Gain (NDCG) at k as proposed by Burges et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/icml/BurgesSRLDHH05,\n    author    = {Christopher J. C. Burges and\n                Tal Shaked and\n                Erin Renshaw and\n                Ari Lazier and\n                Matt Deeds and\n                Nicole Hamilton and\n                Gregory N. Hullender},\n    title     = {Learning to rank using gradient descent},\n    booktitle = {{ICML}},\n    series    = {{ACM} International Conference Proceeding Series},\n    volume    = {119},\n    pages     = {89--96},\n    publisher = {{ACM}},\n    year      = {2005}\n}\n</code></pre> \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] <p>where,</p> <ul> <li>\\(\\operatorname{DCG}\\) is Discounted Cumulative Gain;</li> <li>\\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possible DCG).</li> </ul>"},{"location":"normalization/","title":"Normalization","text":"<p><code>ranx</code> provides several result lists normalization strategies to be used conjunctly with the fusion methods. Normalization aims at transforming the scores of a result list into new values to make them comparable with those of other normalized result lists, which is mandatory for correctly applying many of the provided fusion methods. The normalization strategy to apply before fusion can be defined through the <code>norm</code> parameter of the functions <code>fuse</code> and <code>optimize_fusion</code> (defaults to <code>min-max</code>).</p> Normalization Strategies Alias Min-Max Norm min-max Max Norm max Sum Norm sum ZMUV Norm zmuv Rank Norm rank Borda Norm borda"},{"location":"normalization/#min-max-norm","title":"Min-Max Norm","text":"<p>Min-Max Norm scales the scores (s) of a result list between 0 and 1, scaling to 0 the minimum score (\\(s_{min}\\)) and 1 the maximum score (\\(s_{max}\\)).</p> \\[ \\operatorname{MinMaxNorm(s)}=\\frac{s - s_{min}}{s_{max} - s_{min}} \\]"},{"location":"normalization/#max-norm","title":"Max Norm","text":"<p>Max Norm scales the scores (s) of a result list the maximum score (\\(s_{max}\\)) is scaled to 1.</p> \\[ \\operatorname{MaxNorm(s)}=\\frac{s}{s_{max}} \\]"},{"location":"normalization/#sum-norm","title":"Sum Norm","text":"<p>Sum Norm scales the minimum score (\\(s_{min}\\)) to 0 and the scores sum to 1. It is computed as follows:</p> \\[ \\operatorname{SumNorm(s)}=\\frac{s - s_{min}}{\\sum_s{s - s_{min}}} \\]"},{"location":"normalization/#zmuv-norm","title":"ZMUV Norm","text":"<p>ZMUV Norm (zero-mean, unit-variance) scales the scores so that their mean (\\(s_{mean}\\)) becomes zero and their variance 1.</p> \\[ \\operatorname{ZMUVNorm(s)}=\\frac{s - s_{mean}}{s_{std}} \\]"},{"location":"normalization/#rank-norm","title":"Rank Norm","text":"<p>Rank Norm transforms the scores according to the position in the ranking of the results they are associated with. In this case, the normalized scores are uniformly distributed. The top-ranked result gets a score of 1, while the bottom-ranked result gets a score of \\(\\frac{1}{|r|}\\), where \\(|r|\\) is the number of results in the ranked list.</p> \\[ \\operatorname{RankNorm(s_i)}=1-\\frac{r_i - 1}{|r|} \\]"},{"location":"normalization/#borda-norm","title":"Borda Norm","text":"<p>Borda Norm transforms the scores in a similar manner of how BordaFuse assign points to the results before fusing multiple runs. Borda Norm is defined as follows:</p> \\[ \\operatorname{BordaNorm(s_i)}= \\begin{cases}     1 - \\frac{r_i - 1}{|candidates|} &amp; \\mathit{if}\\ d \\in r \\\\     \\frac{1}{2} - \\frac{|r|-1}{2 \\cdot |candidates|} &amp; \\mathit{otherwise} \\end{cases} \\] <p>Please, refer to Renda et al. for further details.</p>"},{"location":"qrels/","title":"Qrels","text":"<p><code>Qrels</code>, or query relevance judgments, stores the ground truth for conducting evaluations. The preferred way for creating a <code>Qrels</code> instance is converting Python dictionary as follows:</p> <pre><code>from ranx import Qrels\n\nqrels_dict = {\n    \"q_1\": {\n        \"d_1\": 1,\n        \"d_2\": 2,\n    },\n    \"q_2\": {\n        \"d_3\": 2,\n        \"d_2\": 1,\n        \"d_5\": 3,\n    },\n}\n\nqrels = Qrels(qrels_dict, name=\"MSMARCO\")\n</code></pre> <p>Qrels can also be loaded from TREC-style and JSON files, from ir-datasets, and from Pandas DataFrames.</p>"},{"location":"qrels/#load-from-files","title":"Load from files","text":"<p>Parse a qrels file into <code>ranx.Qrels</code>. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, <code>.gz</code> -&gt; <code>gzipped trec</code>. Use the <code>kind</code> argument to override the default behavior.</p> <pre><code>qrels = Qrels.from_file(\"path/to/qrels.json\")  # JSON file\nqrels = Qrels.from_file(\"path/to/qrels.trec\")  # TREC-Style file\nqrels = Qrels.from_file(\"path/to/qrels.txt\")   # TREC-Style file with txt extension\nqrels = Qrels.from_file(\"path/to/qrels.gz\")    # Gzipped TREC-Style file\nqrels = Qrels.from_file(\"path/to/qrels.custom\", kind=\"json\")  # Loaded as JSON file\n</code></pre>"},{"location":"qrels/#load-from-ir-datasets","title":"Load from ir-datasets","text":"<p>You can find the full list of the qrels provided by ir-datasets here.</p> <pre><code>qrels = Qrels.from_ir_datasets(\"msmarco-document/dev\")\n</code></pre>"},{"location":"qrels/#load-from-pandas-dataframes","title":"Load from Pandas DataFrames","text":"<pre><code>from pandas import DataFrame\n\nqrels_df = DataFrame.from_dict({\n    \"q_id\":   [ \"q_1\",  \"q_1\",  \"q_2\",  \"q_2\"  ],\n    \"doc_id\": [ \"d_12\", \"d_25\", \"d_11\", \"d_22\" ],\n    \"score\":  [  5,      3,      6,      1     ],\n})\n\nqrels = Qrels.from_df(\n    df=qrels_df,\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n)\n</code></pre>"},{"location":"qrels/#load-from-parquet-files","title":"Load from Parquet files","text":"<p><code>ranx</code> can load <code>qrels</code> from Parquet files, even from remote sources. You can control the behavior of the underlying <code>pandas.read_parquet</code> function by passing additional arguments through the <code>pd_kwargs</code> argument (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html).</p> <pre><code>qrels = Qrels.from_parquet(\n    path=\"/path/to/parquet/file\"\"\",\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n    pd_kwargs=None,\n)\n</code></pre>"},{"location":"qrels/#save","title":"Save","text":"<p>Write <code>qrels</code> to <code>path</code> as JSON file or TREC qrels format. File type is automatically inferred form the filename extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, <code>.parq</code> -&gt; <code>parquet</code>, <code>.parquet</code> -&gt; <code>parquet</code>. Use the <code>kind</code> argument to override the default behavior.</p> <pre><code>qrels.save(\"path/to/qrels.json\")     # Save as JSON file\nqrels.save(\"path/to/qrels.trec\")     # Save as TREC-Style file\nqrels.save(\"path/to/qrels.txt\")      # Save as TREC-Style file with txt extension\nqrels.save(\"path/to/qrels.parq\")     # Save as Parquet file\nqrels.save(\"path/to/qrels.parquet\")  # Save as Parquet file\nqrels.save(\"path/to/qrels.custom\", kind=\"json\")  # Save as JSON file\n</code></pre>"},{"location":"report/","title":"Report","text":"<p>A <code>Report</code> instance is automatically generated as the results of a comparison. A <code>Report</code> provides a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications. By changing the values of the parameters <code>rounding_digits</code> (int) and <code>show_percentages</code> (bool) you can control what is shown on printing and when generating LaTeX tables.</p> <p><pre><code>from ranx import compare\n\n# Compare different runs and perform statistical tests\nreport = compare(\n    qrels=qrels,\n    runs=[run_1, run_2, run_3, run_4, run_5],\n    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n    max_p=0.01  # P-value threshold\n)\n\nprint(report)\n</code></pre> Output: <pre><code>#    Model    MAP@100     MRR@100     NDCG@10\n---  -------  ----------  ----------  ----------\na    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\nb    model_2  0.2332      0.2339      0.239\nc    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\nd    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\ne    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n</code></pre> <pre><code>print(report.to_latex())  # To get the LaTeX code\n</code></pre></p>"},{"location":"run/","title":"Run","text":"<p><code>Run</code> stores the relevance scores estimated by the model under evaluation. There is no constraint on the score values, i.e., zero and negative scores are not removed.  The preferred way for creating a <code>Run</code> instance is converting a Python dictionary as follows:</p> <pre><code>from ranx import Run\n\nrun_dict = {\n    \"q_1\": {\n        \"d_1\": 1.5,\n        \"d_2\": 2.6,\n    },\n    \"q_2\": {\n        \"d_3\": 2.8,\n        \"d_2\": 1.2,\n        \"d_5\": 3.1,\n    },\n}\n\nrun = Run(run_dict, name=\"bm25\")\n</code></pre> <p><code>Runs</code> can also be loaded from TREC-style and JSON files, and from Pandas DataFrames.</p>"},{"location":"run/#load-from-files","title":"Load from Files","text":"<p>Parse a run file into <code>ranx.Run</code>. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, <code>.gz</code> -&gt; <code>trec</code>, <code>.lz4</code> -&gt; <code>lz4</code>. Use the argument <code>kind</code> to override the default behavior. Use the argument <code>name</code> to set the name of the run. Default is <code>None</code>.</p> <pre><code>run = Run.from_file(\"path/to/run.json\")  # JSON file\nrun = Run.from_file(\"path/to/run.trec\")  # TREC-Style file\nrun = Run.from_file(\"path/to/run.txt\")   # TREC-Style file with txt extension\nrun = Run.from_file(\"path/to/run.gz\")    # Gzipped TREC-Style file\nrun = Run.from_file(\"path/to/run.lz4\")    # lz4 file produced by saving a ranx.Run as lz4\nrun = Run.from_file(\"path/to/run.custom\", kind=\"json\")  # Loaded as JSON file\n</code></pre>"},{"location":"run/#load-from-pandas-dataframes","title":"Load from Pandas DataFrames","text":"<p><code>ranx</code> can load <code>runs</code> from Pandas DataFrames. The argument <code>name</code> is used to set the name of the run. Default is <code>None</code>.</p> <pre><code>from pandas import DataFrame\n\nrun_df = DataFrame.from_dict({\n    \"q_id\":   [ \"q_1\",  \"q_1\",  \"q_2\",  \"q_2\"  ],\n    \"doc_id\": [ \"d_12\", \"d_25\", \"d_11\", \"d_22\" ],\n    \"score\":  [  0.5,    0.3,    0.6,    0.1   ],\n})\n\nrun = Run.from_df(\n    df=run_df,\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n    name=\"my_run\",\n)\n</code></pre>"},{"location":"run/#load-from-parquet-files","title":"Load from Parquet files","text":"<p><code>ranx</code> can load <code>runs</code> from Parquet files, even from remote sources. You can control the behavior of the underlying <code>pandas.read_parquet</code> function by passing additional arguments through the <code>pd_kwargs</code> argument (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). The argument <code>name</code> is used to set the name of the run. Default is <code>None</code>.</p> <pre><code>run = Run.from_parquet(\n    path=\"/path/to/parquet/file\"\"\",\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n    pd_kwargs=None,\n    name=\"my_run\",\n)\n</code></pre>"},{"location":"run/#save","title":"Save","text":"<p>Write <code>run</code> to <code>path</code> as JSON file, TREC run, LZ4 file, or Parquet file.  File type is automatically inferred form the filename extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, and <code>.lz4</code> -&gt; <code>lz4</code>, <code>.parq</code> -&gt; <code>parquet</code>, <code>.parquet</code> -&gt; <code>parquet</code>. Use the <code>kind</code> argument to override this behavior.</p> <pre><code>run.save(\"path/to/run.json\")     # Save as JSON file\nrun.save(\"path/to/run.trec\")     # Save as TREC-Style file\nrun.save(\"path/to/run.txt\")      # Save as TREC-Style file with txt extension\nrun.save(\"path/to/run.lz4\")      # Save as lz4 file\nrun.save(\"path/to/run.parq\")     # Save as Parquet file\nrun.save(\"path/to/run.parquet\")  # Save as Parquet file\nrun.save(\"path/to/run.custom\", kind=\"json\")  # Save as JSON file\n</code></pre>"},{"location":"run/#make-comparable","title":"Make comparable","text":"<p>It adds empty results for queries missing from the run and removes those not appearing in qrels.</p> <pre><code>run.make_comparable(qrels)\n</code></pre>"},{"location":"stat_tests/","title":"Statistical Tests","text":"<p><code>ranx</code> provides two statistical tests that can be used when comparing different runs:  </p> <ul> <li>Fisher's Randomization Test</li> <li>Two-sided Paired Student's t-Test</li> </ul> <p>Please, refer to Smucker et al. for additional information on statistical tests for Information Retrieval.</p> <p>To use the Fisher's Randomization Test, pass <code>stat_test=\"fisher\"</code> to compare.</p> <p>To use the Two-sided Paired Student's t-Test, pass <code>stat_test=\"student\"</code> to compare.</p>"}]}